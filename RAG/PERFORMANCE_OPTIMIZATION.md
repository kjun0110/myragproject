# ì„±ëŠ¥ ìµœì í™” ì „ëµ

## í˜„ì¬ ìƒí™© ë¶„ì„

### ë°œê²¬ëœ ì„±ëŠ¥ ë³‘ëª© ì§€ì 

1. **ëª¨ë¸ ë¡œë”©**
   - ì—¬ëŸ¬ ê³³ì—ì„œ ì¤‘ë³µ ë¡œë”© ê°€ëŠ¥ì„±
   - Lazy loadingì€ ìˆì§€ë§Œ ìµœì í™” ë¶€ì¡±
   - ì „ì—­ ìºì‹±ë§Œ ìˆê³  ë©”ëª¨ë¦¬ ê´€ë¦¬ ë¶€ì¡±

2. **ì¶”ë¡  ì†ë„**
   - ë°°ì¹˜ ì²˜ë¦¬ ë¯¸ì ìš©
   - KV ìºì‹œ ë¯¸í™œìš©
   - ì–‘ìí™” ì„¤ì • ìµœì í™” í•„ìš”

3. **RAG ì²´ì¸**
   - ë²¡í„° ê²€ìƒ‰ ìµœì í™” í•„ìš”
   - ë¬¸ì„œ ì„ë² ë”© ìºì‹± ë¶€ì¡±

---

## ìµœì í™” ì „ëµ (ìš°ì„ ìˆœìœ„ë³„)

### ğŸ”¥ 1ë‹¨ê³„: ì¦‰ì‹œ ì ìš© ê°€ëŠ¥ (High Impact, Low Effort)

#### 1.1 ëª¨ë¸ í”„ë¦¬ë¡œë”© ë° ì‹±ê¸€í†¤ ê°•í™”

**í˜„ì¬ ë¬¸ì œ:**
- ëª¨ë¸ì´ ì²« ìš”ì²­ ì‹œ ë¡œë“œë˜ì–´ ì§€ì—° ë°œìƒ
- ì—¬ëŸ¬ ìš”ì²­ì´ ë™ì‹œì— ëª¨ë¸ ë¡œë”© ì‹œë„ ê°€ëŠ¥

**í•´ê²°ì±…:**
```python
# app/domains/v1/chat/agents/model_loader.py ê°œì„ 
import threading
from functools import lru_cache

_model_lock = threading.Lock()

@lru_cache(maxsize=1)
def load_exaone_model_for_service(...):
    """ì‹±ê¸€í†¤ + ìŠ¤ë ˆë“œ ì•ˆì „ + LRU ìºì‹œ"""
    global _exaone_llm
    if _exaone_llm is not None:
        return _exaone_llm
    
    with _model_lock:
        if _exaone_llm is None:
            # ë¡œë”© ë¡œì§
            ...
    return _exaone_llm
```

**ì˜ˆìƒ íš¨ê³¼:** ì²« ìš”ì²­ ì§€ì—° ì œê±°, ì¤‘ë³µ ë¡œë”© ë°©ì§€

---

#### 1.2 ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ëª¨ë¸ í”„ë¦¬ë¡œë”©

**í˜„ì¬:** Lazy loadingìœ¼ë¡œ ì²« ìš”ì²­ ì‹œ ëŠë¦¼

**í•´ê²°ì±…:**
```python
# app/main.py ë˜ëŠ” startup ì´ë²¤íŠ¸
@app.on_event("startup")
async def preload_models():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ"""
    import asyncio
    from app.domains.v1.chat.agents.model_loader import load_exaone_model_for_service
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¹„ë™ê¸° ë¡œë”©
    asyncio.create_task(
        asyncio.to_thread(load_exaone_model_for_service)
    )
```

**ì˜ˆìƒ íš¨ê³¼:** ì²« ìš”ì²­ ì§€ì—° 0ì´ˆ (ì´ë¯¸ ë¡œë“œë¨)

---

#### 1.3 KV ìºì‹œ í™œì„±í™”

**í˜„ì¬:** ë§¤ë²ˆ ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ì¬ê³„ì‚°

**í•´ê²°ì±…:**
```python
# model_loader.py
base_model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map=device_map,
    trust_remote_code=trust_remote_code,
    use_cache=True,  # KV ìºì‹œ í™œì„±í™”
)

# ì¶”ë¡  ì‹œ
outputs = model.generate(
    **inputs,
    past_key_values=past_key_values,  # ì´ì „ ëŒ€í™” ìºì‹œ ì¬ì‚¬ìš©
    use_cache=True,
)
```

**ì˜ˆìƒ íš¨ê³¼:** ëŒ€í™” ì—°ì†ì„± ì‹œ 30-50% ì†ë„ í–¥ìƒ

---

### âš¡ 2ë‹¨ê³„: ì¤‘ê¸° ìµœì í™” (High Impact, Medium Effort)

#### 2.1 ë°°ì¹˜ ì²˜ë¦¬ êµ¬í˜„

**í˜„ì¬:** ìš”ì²­ë³„ ê°œë³„ ì²˜ë¦¬

**í•´ê²°ì±…:**
```python
# app/domains/v1/chat/agents/chat_service.py
from collections import deque
import asyncio

class BatchProcessor:
    def __init__(self, batch_size=4, max_wait=0.1):
        self.batch_size = batch_size
        self.max_wait = max_wait
        self.queue = deque()
        self.lock = asyncio.Lock()
    
    async def process(self, inputs):
        """ë°°ì¹˜ë¡œ ë¬¶ì–´ì„œ ì²˜ë¦¬"""
        async with self.lock:
            self.queue.append(inputs)
            if len(self.queue) >= self.batch_size:
                batch = [self.queue.popleft() for _ in range(self.batch_size)]
                return await self._process_batch(batch)
        
        # íƒ€ì„ì•„ì›ƒ ëŒ€ê¸°
        await asyncio.sleep(self.max_wait)
        # ë°°ì¹˜ ì²˜ë¦¬
```

**ì˜ˆìƒ íš¨ê³¼:** ë™ì‹œ ìš”ì²­ ì‹œ 2-4ë°° ì²˜ë¦¬ëŸ‰ ì¦ê°€

---

#### 2.2 ì–‘ìí™” ìµœì í™”

**í˜„ì¬:** 4-bit ì–‘ìí™” ì‚¬ìš© ì¤‘

**ìµœì í™” ì˜µì…˜:**
```python
# ë” ë¹ ë¥¸ ì–‘ìí™” ì„¤ì •
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,  # bfloat16 â†’ float16 (ë” ë¹ ë¦„)
    bnb_4bit_use_double_quant=True,
)

# ë˜ëŠ” 8-bitë¡œ ë³€ê²½ (ì†ë„ vs ë©”ëª¨ë¦¬ íŠ¸ë ˆì´ë“œì˜¤í”„)
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,  # 4-bitë³´ë‹¤ ë¹ ë¦„
)
```

**ì˜ˆìƒ íš¨ê³¼:** ì¶”ë¡  ì†ë„ 20-30% í–¥ìƒ

---

#### 2.3 Flash Attention 2 ì‚¬ìš©

**í˜„ì¬:** í‘œì¤€ Attention ì‚¬ìš©

**í•´ê²°ì±…:**
```python
# model_loader.py
base_model = AutoModelForCausalLM.from_pretrained(
    model_path,
    attn_implementation="flash_attention_2",  # Flash Attention 2
    device_map=device_map,
    trust_remote_code=trust_remote_code,
)
```

**í•„ìš” íŒ¨í‚¤ì§€:**
```bash
pip install flash-attn --no-build-isolation
```

**ì˜ˆìƒ íš¨ê³¼:** ê¸´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ 2-3ë°° ì†ë„ í–¥ìƒ

---

#### 2.4 í…ìŠ¤íŠ¸ ìƒì„± ìµœì í™”

**í˜„ì¬:** ê¸°ë³¸ ìƒì„± ì„¤ì •

**ìµœì í™”:**
```python
# ë” ë¹ ë¥¸ ìƒì„± íŒŒë¼ë¯¸í„°
outputs = model.generate(
    **inputs,
    max_new_tokens=max_new_tokens,
    do_sample=False,  # Greedy decoding (ë” ë¹ ë¦„)
    num_beams=1,  # Beam search ë¹„í™œì„±í™”
    use_cache=True,
    pad_token_id=tokenizer.eos_token_id,
)
```

**ì˜ˆìƒ íš¨ê³¼:** ìƒì„± ì†ë„ 20-40% í–¥ìƒ

---

### ğŸš€ 3ë‹¨ê³„: ì¥ê¸° ìµœì í™” (High Impact, High Effort)

#### 3.1 ëª¨ë¸ ì„œë¹™ í”„ë ˆì„ì›Œí¬ ë„ì…

**ì˜µì…˜ 1: vLLM**
```python
# vLLMì€ ë°°ì¹˜ ì²˜ë¦¬, KV ìºì‹œ ìµœì í™” ìë™ ì œê³µ
from vllm import LLM

llm = LLM(
    model="LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct",
    quantization="awq",  # ë˜ëŠ” "gptq"
    max_model_len=32768,
    gpu_memory_utilization=0.9,
)
```

**ì˜µì…˜ 2: TensorRT-LLM**
- NVIDIA GPU ì „ìš©
- ìµœê³  ì„±ëŠ¥ (2-5ë°° í–¥ìƒ)

**ì˜ˆìƒ íš¨ê³¼:** ì „ì²´ ì²˜ë¦¬ëŸ‰ 3-5ë°° ì¦ê°€

---

#### 3.2 ë¹„ë™ê¸° ì¶”ë¡  íŒŒì´í”„ë¼ì¸

**í˜„ì¬:** ë™ê¸° ì²˜ë¦¬

**í•´ê²°ì±…:**
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncModelInference:
    def __init__(self, model, tokenizer, max_workers=2):
        self.model = model
        self.tokenizer = tokenizer
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    async def generate_async(self, prompt, **kwargs):
        """ë¹„ë™ê¸° ì¶”ë¡ """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor,
            self._generate_sync,
            prompt,
            **kwargs
        )
```

**ì˜ˆìƒ íš¨ê³¼:** ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ëŠ¥ë ¥ í–¥ìƒ

---

#### 3.3 ë²¡í„° ê²€ìƒ‰ ìµœì í™”

**í˜„ì¬:** PGVector ì‚¬ìš©

**ìµœì í™”:**
```python
# HNSW ì¸ë±ìŠ¤ ì‚¬ìš© (ë” ë¹ ë¥¸ ê²€ìƒ‰)
vector_store = PGVector(
    connection_string=connection_string,
    collection_name=collection_name,
    embedding_function=embeddings,
    use_jsonb=True,
    pre_delete_collection=False,
)

# ì¸ë±ìŠ¤ ìƒì„±
CREATE INDEX ON vector_store USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);
```

**ì˜ˆìƒ íš¨ê³¼:** ê²€ìƒ‰ ì†ë„ 5-10ë°° í–¥ìƒ

---

#### 3.4 ë¬¸ì„œ ì„ë² ë”© ìºì‹±

**í˜„ì¬:** ë§¤ë²ˆ ì¬ì„ë² ë”©

**í•´ê²°ì±…:**
```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=10000)
def get_cached_embedding(text: str, model_name: str):
    """ì„ë² ë”© ìºì‹±"""
    return embeddings.embed_query(text)

# ë˜ëŠ” Redis ìºì‹œ
import redis
r = redis.Redis()

def get_embedding(text):
    key = f"embedding:{hashlib.md5(text.encode()).hexdigest()}"
    cached = r.get(key)
    if cached:
        return pickle.loads(cached)
    embedding = embeddings.embed_query(text)
    r.setex(key, 3600, pickle.dumps(embedding))  # 1ì‹œê°„ ìºì‹œ
    return embedding
```

**ì˜ˆìƒ íš¨ê³¼:** ë°˜ë³µ ë¬¸ì„œ ê²€ìƒ‰ ì‹œ 10-100ë°° ë¹ ë¦„

---

## êµ¬í˜„ ìš°ì„ ìˆœìœ„

### Phase 1 (1ì£¼ì¼ ë‚´)
1. âœ… ëª¨ë¸ í”„ë¦¬ë¡œë”© (startup ì´ë²¤íŠ¸)
2. âœ… KV ìºì‹œ í™œì„±í™”
3. âœ… ì‹±ê¸€í†¤ + ìŠ¤ë ˆë“œ ì•ˆì „ ê°•í™”
4. âœ… ìƒì„± íŒŒë¼ë¯¸í„° ìµœì í™”

**ì˜ˆìƒ íš¨ê³¼:** ì²« ìš”ì²­ ì§€ì—° ì œê±°, 20-30% ì†ë„ í–¥ìƒ

### Phase 2 (2-3ì£¼ ë‚´)
1. âš¡ Flash Attention 2
2. âš¡ ì–‘ìí™” ìµœì í™” (float16)
3. âš¡ ë°°ì¹˜ ì²˜ë¦¬ ê¸°ë³¸ êµ¬í˜„
4. âš¡ ë²¡í„° ê²€ìƒ‰ ì¸ë±ìŠ¤ ìµœì í™”

**ì˜ˆìƒ íš¨ê³¼:** ì¶”ê°€ 30-50% ì†ë„ í–¥ìƒ

### Phase 3 (1-2ê°œì›” ë‚´)
1. ğŸš€ vLLM ë˜ëŠ” TensorRT-LLM ë„ì…
2. ğŸš€ ë¹„ë™ê¸° ì¶”ë¡  íŒŒì´í”„ë¼ì¸
3. ğŸš€ ë¬¸ì„œ ì„ë² ë”© ìºì‹± (Redis)
4. ğŸš€ ëª¨ë‹ˆí„°ë§ ë° í”„ë¡œíŒŒì¼ë§

**ì˜ˆìƒ íš¨ê³¼:** ì „ì²´ 3-5ë°° ì„±ëŠ¥ í–¥ìƒ

---

## ëª¨ë‹ˆí„°ë§ ì§€í‘œ

### ì¸¡ì •í•  ë©”íŠ¸ë¦­

1. **ëª¨ë¸ ë¡œë”© ì‹œê°„**
   - ì²« ë¡œë”©: ëª©í‘œ < 30ì´ˆ
   - ìºì‹œëœ ë¡œë”©: ëª©í‘œ < 1ì´ˆ

2. **ì¶”ë¡  ì†ë„**
   - í† í°/ì´ˆ: ëª©í‘œ > 20 tokens/s
   - ì²« í† í° ì§€ì—°: ëª©í‘œ < 500ms
   - ì „ì²´ ìƒì„± ì‹œê°„: ëª©í‘œ < 5ì´ˆ (100 í† í° ê¸°ì¤€)

3. **ë™ì‹œ ì²˜ë¦¬**
   - ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ìˆ˜: ëª©í‘œ > 4
   - í‰ê·  ì‘ë‹µ ì‹œê°„: ëª©í‘œ < 3ì´ˆ

4. **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**
   - GPU ë©”ëª¨ë¦¬: ëª©í‘œ < 80%
   - ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: ëª©í‘œ < 70%

---

## í•˜ë“œì›¨ì–´ ìµœì í™”

### GPU ì„¤ì •
```python
# CUDA ìµœì í™”
import torch
torch.backends.cudnn.benchmark = True  # cuDNN ìë™ íŠœë‹
torch.backends.cuda.matmul.allow_tf32 = True  # TF32 í™œì„±í™”
```

### í™˜ê²½ ë³€ìˆ˜
```bash
# .env
CUDA_LAUNCH_BLOCKING=0  # ë¹„ë™ê¸° ì‹¤í–‰
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512  # ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë°©ì§€
```

---

## ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸

### í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

```python
# benchmarks/test_performance.py
import time
import asyncio

async def benchmark_model():
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    from app.common.loaders import load_exaone_with_spam_adapter
    
    # ë¡œë”© ì‹œê°„
    start = time.time()
    model, tokenizer = load_exaone_with_spam_adapter()
    load_time = time.time() - start
    print(f"ë¡œë”© ì‹œê°„: {load_time:.2f}ì´ˆ")
    
    # ì¶”ë¡  ì†ë„
    prompt = "ì•ˆë…•í•˜ì„¸ìš”" * 10
    start = time.time()
    outputs = model.generate(**tokenizer(prompt, return_tensors="pt"))
    inference_time = time.time() - start
    tokens = len(outputs[0])
    print(f"ì¶”ë¡  ì†ë„: {tokens/inference_time:.2f} tokens/s")
    
    # ë™ì‹œ ìš”ì²­
    async def concurrent_request(i):
        start = time.time()
        outputs = model.generate(**tokenizer(f"ì§ˆë¬¸ {i}", return_tensors="pt"))
        return time.time() - start
    
    times = await asyncio.gather(*[concurrent_request(i) for i in range(5)])
    print(f"ë™ì‹œ ìš”ì²­ í‰ê· : {sum(times)/len(times):.2f}ì´ˆ")
```

---

## ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

| ë‹¨ê³„ | í˜„ì¬ | Phase 1 | Phase 2 | Phase 3 |
|------|------|---------|---------|---------|
| ì²« ìš”ì²­ ì§€ì—° | 30-60ì´ˆ | 0ì´ˆ | 0ì´ˆ | 0ì´ˆ |
| ì¶”ë¡  ì†ë„ | 10-15 tokens/s | 15-20 tokens/s | 25-35 tokens/s | 50-100 tokens/s |
| ë™ì‹œ ì²˜ë¦¬ | 1-2 ìš”ì²­ | 2-4 ìš”ì²­ | 4-8 ìš”ì²­ | 10-20 ìš”ì²­ |
| ë©”ëª¨ë¦¬ ì‚¬ìš© | 100% | 90% | 85% | 80% |

---

## ë‹¤ìŒ ë‹¨ê³„

1. **ì¦‰ì‹œ ì‹œì‘:** Phase 1 êµ¬í˜„ (í”„ë¦¬ë¡œë”©, KV ìºì‹œ)
2. **ëª¨ë‹ˆí„°ë§ ì„¤ì •:** ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘
3. **ì ì§„ì  ê°œì„ :** Phase 2, 3 ìˆœì°¨ êµ¬í˜„
4. **ì§€ì†ì  ìµœì í™”:** ë²¤ì¹˜ë§ˆí¬ ê¸°ë°˜ íŠœë‹
