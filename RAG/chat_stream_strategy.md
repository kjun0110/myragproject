# ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„ ì „ëµ ë¬¸ì„œ (Chat Stream Strategy)

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [ì•„í‚¤í…ì²˜ ê°œìš”](#ì•„í‚¤í…ì²˜-ê°œìš”)
3. [ë°±ì—”ë“œ ìŠ¤íŠ¸ë¦¬ë° ì „ëµ](#ë°±ì—”ë“œ-ìŠ¤íŠ¸ë¦¬ë°-ì „ëµ)
4. [í”„ë¡ íŠ¸ì—”ë“œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬](#í”„ë¡ íŠ¸ì—”ë“œ-ìŠ¤íŠ¸ë¦¬ë°-ì²˜ë¦¬)
5. [í† í° ë‹¨ìœ„ ì¦ë¶„ ì—…ë°ì´íŠ¸](#í† í°-ë‹¨ìœ„-ì¦ë¶„-ì—…ë°ì´íŠ¸)
6. [Next.js API ë¼ìš°íŠ¸ í”„ë¡ì‹œ íŒ¨í„´](#nextjs-api-ë¼ìš°íŠ¸-í”„ë¡ì‹œ-íŒ¨í„´)
7. [êµ¬í˜„ ì„¸ë¶€ì‚¬í•­](#êµ¬í˜„-ì„¸ë¶€ì‚¬í•­)
8. [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)
9. [ë¬¸ì œ í•´ê²° ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#ë¬¸ì œ-í•´ê²°-ë°-íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

---

## ê°œìš”

ë³¸ ë¬¸ì„œëŠ” LangChain/LangGraph ê¸°ë°˜ ì±—ë´‡ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ **ì‹¤ì‹œê°„ í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°**ì„ êµ¬í˜„í•˜ê¸° ìœ„í•œ ì „ëµì„ ì„¤ëª…í•©ë‹ˆë‹¤.

### í•µì‹¬ ëª©í‘œ
- âœ… **í† í° ë‹¨ìœ„ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**: LLMì´ ìƒì„±í•˜ëŠ” ê° í† í°ì„ ì¦‰ì‹œ í”„ë¡ íŠ¸ì—”ë“œì— ì „ë‹¬
- âœ… **ì¦ë¶„ ì—…ë°ì´íŠ¸**: ì „ì²´ ë©”ì‹œì§€ê°€ ì•„ë‹Œ ìƒˆë¡œ ìƒì„±ëœ í† í°(delta)ë§Œ ì „ì†¡
- âœ… **ê¹”ë”í•œ ì‘ë‹µ**: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ë‚´ë¶€ íƒœê·¸, ê³¼ê±° ëŒ€í™” ë‚´ìš© ì œê±°
- âœ… **ë¶€ë“œëŸ¬ìš´ UX**: í•œ ê¸€ìì”© íƒ€ì´í•‘ë˜ëŠ” ë“¯í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì‚¬ìš©ì ê²½í—˜

---

## ì•„í‚¤í…ì²˜ ê°œìš”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚
â”‚  (Next.js)      â”‚
â”‚  page.tsx       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP POST
         â”‚ /api/chat or /api/graph
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Next.js API    â”‚
â”‚  Route Proxy    â”‚
â”‚  route.ts       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP POST
         â”‚ text/plain stream
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Backend       â”‚
â”‚  (FastAPI)      â”‚
â”‚  router.py     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â–º LangGraph (astream_events)
         â”‚   â””â”€â–º on_chat_model_stream
         â”‚
         â””â”€â–º RAG Chain (astream)
             â””â”€â–º answer chunks
```

### ë°ì´í„° íë¦„
1. **í”„ë¡ íŠ¸ì—”ë“œ** â†’ Next.js API ë¼ìš°íŠ¸ë¡œ ìš”ì²­
2. **Next.js API ë¼ìš°íŠ¸** â†’ FastAPI ë°±ì—”ë“œë¡œ í”„ë¡ì‹œ
3. **FastAPI ë°±ì—”ë“œ** â†’ LangGraph/RAG Chainì—ì„œ ìŠ¤íŠ¸ë¦¬ë°
4. **ìŠ¤íŠ¸ë¦¼** â†’ í† í° ë‹¨ìœ„ë¡œ `text/plain` ì „ì†¡
5. **í”„ë¡ íŠ¸ì—”ë“œ** â†’ `ReadableStream`ìœ¼ë¡œ ìˆ˜ì‹ í•˜ì—¬ ì‹¤ì‹œê°„ ë Œë”ë§

---

## ë°±ì—”ë“œ ìŠ¤íŠ¸ë¦¬ë° ì „ëµ

### 1. LangGraph ìŠ¤íŠ¸ë¦¬ë° (`graph_router.py`)

#### ì „ëµ: `astream_events` ì‚¬ìš©
LangGraphì˜ `astream_events` APIë¥¼ ì‚¬ìš©í•˜ì—¬ **ì´ë²¤íŠ¸ ê¸°ë°˜ í† í° ìŠ¤íŠ¸ë¦¬ë°**ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

```python
async def stream_generator():
    """ìŠ¤íŠ¸ë¦¬ë° ì œë„ˆë ˆì´í„° - í† í° ë‹¨ìœ„ ì¦ë¶„ ì—…ë°ì´íŠ¸."""
    # astream_eventsë¥¼ ì‚¬ìš©í•˜ì—¬ í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°
    async for event in graph.astream_events(state, version="v1"):
        event_type = event.get("event")

        # on_chat_model_stream ì´ë²¤íŠ¸ë§Œ ì²˜ë¦¬ (í† í° ë‹¨ìœ„)
        if event_type == "on_chat_model_stream":
            chunk_data = event.get("data", {})
            chunk = chunk_data.get("chunk")

            if chunk and hasattr(chunk, "content"):
                content = chunk.content
                if content:
                    # íƒœê·¸ ì œê±° (ì‹¤ì‹œê°„ìœ¼ë¡œ)
                    if not any(tag in content for tag in ["[[system]]", "[[endofturn]]"]):
                        yield content  # ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ yield
                        await asyncio.sleep(0.01)  # 10ms ì§€ì—°
```

#### í•µì‹¬ í¬ì¸íŠ¸
- âœ… **ì´ë²¤íŠ¸ í•„í„°ë§**: `on_chat_model_stream` ì´ë²¤íŠ¸ë§Œ ì²˜ë¦¬
- âœ… **í† í° ë‹¨ìœ„ ì „ì†¡**: `chunk.content`ë§Œ yield (ì „ì²´ ë©”ì‹œì§€ ì•„ë‹˜)
- âœ… **ì‹¤ì‹œê°„ í•„í„°ë§**: ë‚´ë¶€ íƒœê·¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì— ì œê±°
- âœ… **ì§€ì—° ì œì–´**: `asyncio.sleep(0.01)`ë¡œ ë¶€ë“œëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼

#### FastAPI ì‘ë‹µ
```python
return StreamingResponse(
    stream_generator(),
    media_type="text/plain; charset=utf-8",  # JSONì´ ì•„ë‹Œ ìˆœìˆ˜ í…ìŠ¤íŠ¸
)
```

---

### 2. RAG Chain ìŠ¤íŠ¸ë¦¬ë° (`chat_router.py`)

#### ì „ëµ: `astream()` ì‚¬ìš©
LangChain RAG Chainì˜ `astream()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ **ì¦ë¶„ ì—…ë°ì´íŠ¸**ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

```python
async def stream_response():
    accumulated_text = ""

    async for chunk in current_rag_chain.astream({
        "input": request.message,
        "chat_history": chat_history,
    }):
        # chunkì—ì„œ answer ì¶”ì¶œ
        if isinstance(chunk, dict):
            answer = chunk.get("answer", "")
            if answer:
                # ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì¦ë¶„ ì—…ë°ì´íŠ¸)
                if len(answer) > len(accumulated_text):
                    delta = answer[len(accumulated_text):]
                    accumulated_text = answer

                    # í•œ ê¸€ìì”© ìŠ¤íŠ¸ë¦¬ë°
                    for char in delta:
                        yield char
                        await asyncio.sleep(0.01)
```

#### í•µì‹¬ í¬ì¸íŠ¸
- âœ… **ì¦ë¶„ ì¶”ì¶œ**: `delta = answer[len(accumulated_text):]`ë¡œ ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ
- âœ… **ë¬¸ì ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°**: í•œ ê¸€ìì”© yieldí•˜ì—¬ íƒ€ì´í•‘ íš¨ê³¼ êµ¬í˜„
- âœ… **ëˆ„ì  ì¶”ì **: `accumulated_text`ë¡œ ì „ì²´ ê¸¸ì´ ì¶”ì 

---

## í”„ë¡ íŠ¸ì—”ë“œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬

### 1. Next.js API ë¼ìš°íŠ¸ í”„ë¡ì‹œ (`api/chat/route.ts`, `api/graph/route.ts`)

#### ì „ëµ: ì§ì ‘ ìŠ¤íŠ¸ë¦¼ ì „ë‹¬
ë°±ì—”ë“œì—ì„œ ë°›ì€ `text/plain` ìŠ¤íŠ¸ë¦¼ì„ **ê·¸ëŒ€ë¡œ í”„ë¡ íŠ¸ì—”ë“œì— ì „ë‹¬**í•©ë‹ˆë‹¤.

```typescript
// Content-Type í™•ì¸
const contentType = response.headers.get("content-type");

if (contentType && contentType.includes("text/plain")) {
  // ë°±ì—”ë“œì˜ text/plain ìŠ¤íŠ¸ë¦¼ì„ ê·¸ëŒ€ë¡œ í”„ë¡ íŠ¸ì—”ë“œì— ì „ë‹¬
  return new Response(response.body, {
    headers: {
      "Content-Type": "text/plain; charset=utf-8",
      "Cache-Control": "no-cache",
      "Connection": "keep-alive",
    },
  });
}
```

#### í•µì‹¬ í¬ì¸íŠ¸
- âœ… **ìŠ¤íŠ¸ë¦¼ ì§ì ‘ ì „ë‹¬**: JSON ë³€í™˜ ì—†ì´ `response.body`ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬
- âœ… **Content-Type í™•ì¸**: `text/plain`ì¼ ë•Œë§Œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
- âœ… **í—¤ë” ì„¤ì •**: `Cache-Control: no-cache`, `Connection: keep-alive`

#### âŒ í”¼í•´ì•¼ í•  ì‹¤ìˆ˜
```typescript
// ì˜ëª»ëœ ì˜ˆ: JSONìœ¼ë¡œ ê°ì‹¸ê¸°
controller.enqueue(
  new TextEncoder().encode(
    JSON.stringify({ delta: chunk }) + '\n'  // âŒ ì´ë ‡ê²Œ í•˜ë©´ í”„ë¡ íŠ¸ì—”ë“œì— JSON ë¬¸ìì—´ì´ í‘œì‹œë¨
  )
);
```

---

### 2. í”„ë¡ íŠ¸ì—”ë“œ ì»´í¬ë„ŒíŠ¸ (`page.tsx`)

#### ì „ëµ: ReadableStream ì²˜ë¦¬
`ReadableStream`ì„ ì‚¬ìš©í•˜ì—¬ **ì‹¤ì‹œê°„ìœ¼ë¡œ ë©”ì‹œì§€ ìƒíƒœ ì—…ë°ì´íŠ¸**í•©ë‹ˆë‹¤.

```typescript
// ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
if (isStreaming || modelType === "graph") {
  // ìŠ¤íŠ¸ë¦¬ë° ë©”ì‹œì§€ ìƒì„± (ì´ˆê¸° ìƒíƒœ)
  const streamingMessageId = (Date.now() + 1).toString();
  const streamingMessage: Message = {
    id: streamingMessageId,
    role: "assistant",
    content: "",  // ë¹ˆ ë¬¸ìì—´ë¡œ ì‹œì‘
    timestamp: new Date(),
  };
  setMessages((prev) => [...prev, streamingMessage]);

  // ReadableStream ì²˜ë¦¬
  const reader = response.body.getReader();
  const decoder = new TextDecoder();
  let accumulatedText = "";

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    // ì²­í¬ ë””ì½”ë”© (ìˆœìˆ˜ í…ìŠ¤íŠ¸)
    const chunk = decoder.decode(value, { stream: true });

    if (chunk) {
      // ìˆœìˆ˜ í…ìŠ¤íŠ¸ ëˆ„ì 
      accumulatedText += chunk;

      // ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ (ë§ˆì§€ë§‰ ë©”ì‹œì§€ë§Œ ìˆ˜ì •)
      setMessages((prev) => {
        const updated = [...prev];
        const msgIndex = updated.findIndex((m) => m.id === streamingMessageId);
        if (msgIndex !== -1) {
          updated[msgIndex] = {
            ...updated[msgIndex],
            content: accumulatedText,  // ëˆ„ì ëœ í…ìŠ¤íŠ¸ë¡œ ì—…ë°ì´íŠ¸
          };
        }
        return updated;
      });
    }
  }
}
```

#### í•µì‹¬ í¬ì¸íŠ¸
- âœ… **ì´ˆê¸° ë¹ˆ ë©”ì‹œì§€ ìƒì„±**: ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì‹œ ë¹ˆ ë©”ì‹œì§€ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
- âœ… **í…ìŠ¤íŠ¸ ëˆ„ì **: `accumulatedText += chunk`ë¡œ ì²­í¬ë¥¼ ëˆ„ì 
- âœ… **ì¦ë¶„ ì—…ë°ì´íŠ¸**: `setMessages`ë¡œ ë§ˆì§€ë§‰ ë©”ì‹œì§€ë§Œ ì—…ë°ì´íŠ¸
- âœ… **TextDecoder ì‚¬ìš©**: `{ stream: true }` ì˜µì…˜ìœ¼ë¡œ ë©€í‹°ë°”ì´íŠ¸ ë¬¸ì ì²˜ë¦¬

---

## í† í° ë‹¨ìœ„ ì¦ë¶„ ì—…ë°ì´íŠ¸

### ë¬¸ì œ: ì „ì²´ ë©”ì‹œì§€ ì¤‘ë³µ ì „ì†¡

#### ê¸°ì¡´ ë¬¸ì œì 
```python
# âŒ ì˜ëª»ëœ ë°©ì‹: ì „ì²´ ë©”ì‹œì§€ë¥¼ ë§¤ë²ˆ ì „ì†¡
for event in graph.stream(state):
    messages = event.get("messages", [])
    last_message = messages[-1].content  # ì „ì²´ ë©”ì‹œì§€
    yield last_message  # ì¤‘ë³µ ì „ì†¡!
```

#### í•´ê²°: ì¦ë¶„ ì—…ë°ì´íŠ¸
```python
# âœ… ì˜¬ë°”ë¥¸ ë°©ì‹: ìƒˆë¡œ ìƒì„±ëœ í† í°ë§Œ ì „ì†¡
async for event in graph.astream_events(state, version="v1"):
    if event["event"] == "on_chat_model_stream":
        chunk = event["data"]["chunk"]
        yield chunk.content  # ìƒˆ í† í°ë§Œ yield
```

### LangGraph: `astream_events` ì‚¬ìš©

#### ì´ë²¤íŠ¸ íƒ€ì…
- `on_chat_model_start`: ëª¨ë¸ í˜¸ì¶œ ì‹œì‘
- `on_chat_model_stream`: **í† í° ìƒì„± ì´ë²¤íŠ¸** (ì´ê²ƒë§Œ ì‚¬ìš©)
- `on_chat_model_end`: ëª¨ë¸ í˜¸ì¶œ ì¢…ë£Œ

#### êµ¬í˜„ ì˜ˆì‹œ
```python
async for event in graph.astream_events(state, version="v1"):
    event_type = event.get("event")

    if event_type == "on_chat_model_stream":
        chunk = event.get("data", {}).get("chunk")
        if chunk and hasattr(chunk, "content"):
            content = chunk.content
            if content:
                yield content  # í† í° ë‹¨ìœ„ ì „ì†¡
```

### RAG Chain: ì¦ë¶„ ì¶”ì¶œ

#### êµ¬í˜„ ì˜ˆì‹œ
```python
accumulated_text = ""

async for chunk in chain.astream(input_data):
    answer = chunk.get("answer", "")
    if answer and len(answer) > len(accumulated_text):
        # ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        delta = answer[len(accumulated_text):]
        accumulated_text = answer

        # í•œ ê¸€ìì”© ìŠ¤íŠ¸ë¦¬ë°
        for char in delta:
            yield char
            await asyncio.sleep(0.01)
```

---

## Next.js API ë¼ìš°íŠ¸ í”„ë¡ì‹œ íŒ¨í„´

### ì•„í‚¤í…ì²˜ ì´ìœ 

#### ì™œ í”„ë¡ì‹œê°€ í•„ìš”í•œê°€?
1. **CORS ë¬¸ì œ í•´ê²°**: ë°±ì—”ë“œì™€ í”„ë¡ íŠ¸ì—”ë“œê°€ ë‹¤ë¥¸ í¬íŠ¸ì—ì„œ ì‹¤í–‰
2. **í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬**: `NEXT_PUBLIC_*`ë¡œ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ë°±ì—”ë“œ URL ì ‘ê·¼
3. **ì—ëŸ¬ ì²˜ë¦¬ í†µí•©**: Next.jsì—ì„œ í†µì¼ëœ ì—ëŸ¬ ì‘ë‹µ í˜•ì‹
4. **íƒ€ì„ì•„ì›ƒ ê´€ë¦¬**: ë¡œì»¬ ëª¨ë¸ì˜ ê¸´ ì‘ë‹µ ì‹œê°„ ì²˜ë¦¬

### êµ¬í˜„ íŒ¨í„´

#### 1. ìš”ì²­ ì „ë‹¬
```typescript
const response = await fetch(`${backendUrl}/api/graph`, {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({ message, history }),
});
```

#### 2. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
```typescript
if (contentType && contentType.includes("text/plain")) {
  // ìŠ¤íŠ¸ë¦¼ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
  return new Response(response.body, {
    headers: {
      "Content-Type": "text/plain; charset=utf-8",
      "Cache-Control": "no-cache",
      "Connection": "keep-alive",
    },
  });
}
```

#### 3. ì—ëŸ¬ ì²˜ë¦¬
```typescript
if (!response.ok) {
  const errorData = await response.json().catch(() => ({}));
  return NextResponse.json(
    { error: errorData.detail || "ì„œë²„ ì˜¤ë¥˜" },
    { status: response.status }
  );
}
```

---

## êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### 1. ì‘ë‹µ ì •ë¦¬ (Response Cleaning)

#### ë¬¸ì œ: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë° íƒœê·¸ ë…¸ì¶œ
LLM ì‘ë‹µì— `[[system]]`, `[[endofturn]]`, `[[assistant]]` ê°™ì€ ë‚´ë¶€ íƒœê·¸ê°€ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### í•´ê²°: ì •ê·œì‹ ê¸°ë°˜ í•„í„°ë§

**ë°±ì—”ë“œ (`chat_service.py`, `graph.py`)**
```python
import re

# 1. íƒœê·¸ ì œê±°
response_text = re.sub(r'\[\[system\]\].*?\[\[endofturn\]\]\s*', '', response_text, flags=re.DOTALL)
response_text = re.sub(r'\[\[assistant\]\]\s*', '', response_text, flags=re.IGNORECASE)
response_text = re.sub(r'\[\[endofturn\]\]\s*', '', response_text, flags=re.IGNORECASE)
response_text = re.sub(r'\[\[user\]\]\s*', '', response_text, flags=re.IGNORECASE)

# 2. ê³¼ê±° ëŒ€í™” í˜•ì‹ ì œê±° (Human:, Assistant:)
if "Human:" in response_text or "Assistant:" in response_text:
    assistant_match = re.search(r"Assistant:\s*(.+?)(?:\nHuman:|$)", response_text, re.DOTALL)
    if assistant_match:
        response_text = assistant_match.group(1).strip()

# 3. ê°„ë‹¨í•œ ì¸ì‚¬ì— ëŒ€í•œ ì‘ë‹µ ì •ë¦¬
if any(greeting in message.lower() for greeting in ["ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "hi", "hello"]):
    lines = response_text.split('\n')
    clean_lines = []
    for line in lines:
        line = line.strip()
        if not line or line.startswith("Human:") or line.startswith("Assistant:"):
            continue
        clean_lines.append(line)

    if clean_lines:
        response_text = '\n'.join(clean_lines)
    else:
        response_text = "ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ ìˆ˜ ìˆì„ê¹Œìš”? ê¶ê¸ˆí•œ ì ì´ ìˆê±°ë‚˜ ë„ì›€ì´ í•„ìš”í•œ ì‚¬í•­ì´ ìˆìœ¼ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ğŸ˜Š"
```

**ìŠ¤íŠ¸ë¦¬ë° ì¤‘ í•„í„°ë§ (`graph_router.py`)**
```python
# ì‹¤ì‹œê°„ìœ¼ë¡œ íƒœê·¸ ì œê±°
if not any(tag in content for tag in ["[[system]]", "[[endofturn]]", "[[user]]", "[[assistant]]"]):
    yield content
```

---

### 2. ì§€ì—° ì œì–´ (Delay Control)

#### ëª©ì 
- ë¶€ë“œëŸ¬ìš´ íƒ€ì´í•‘ íš¨ê³¼
- ë„¤íŠ¸ì›Œí¬ ë¶€í•˜ ë¶„ì‚°
- ì‚¬ìš©ì ê²½í—˜ ê°œì„ 

#### êµ¬í˜„
```python
await asyncio.sleep(0.01)  # 10ms ì§€ì—°
```

#### ìµœì í™” ê³ ë ¤ì‚¬í•­
- **ë„ˆë¬´ ì§§ìœ¼ë©´**: ë„¤íŠ¸ì›Œí¬ ë¶€í•˜ ì¦ê°€, ë Œë”ë§ ë¹„ìš© ì¦ê°€
- **ë„ˆë¬´ ê¸¸ë©´**: ëŠë¦° ì‚¬ìš©ì ê²½í—˜
- **ê¶Œì¥ê°’**: 10ms (0.01ì´ˆ)

---

### 3. ì—ëŸ¬ ì²˜ë¦¬

#### ë°±ì—”ë“œ ì—ëŸ¬ ì²˜ë¦¬
```python
async def stream_generator():
    try:
        async for event in graph.astream_events(state, version="v1"):
            # ... ìŠ¤íŠ¸ë¦¬ë° ë¡œì§ ...
    except Exception as e:
        print(f"[ERROR] ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜: {e}")
        yield f"\n\n[ì˜¤ë¥˜ ë°œìƒ: {str(e)}]"
```

#### í”„ë¡ íŠ¸ì—”ë“œ ì—ëŸ¬ ì²˜ë¦¬
```typescript
try {
  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    // ... ì²˜ë¦¬ ...
  }
} catch (streamError) {
  console.error("[ERROR] ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹¤íŒ¨:", streamError);
  setMessages((prev) => {
    // ì—ëŸ¬ ë©”ì‹œì§€ë¡œ ì—…ë°ì´íŠ¸
    const updated = [...prev];
    const msgIndex = updated.findIndex((m) => m.id === streamingMessageId);
    if (msgIndex !== -1) {
      updated[msgIndex] = {
        ...updated[msgIndex],
        content: accumulatedText || "âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
      };
    }
    return updated;
  });
}
```

---

## ì„±ëŠ¥ ìµœì í™”

### 1. ë©”ëª¨ë¦¬ ìµœì í™”

#### ë¬¸ì œ: ì „ì²´ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ëˆ„ì 
ê³¼ê±° ëŒ€í™”ê°€ ê³„ì† ëˆ„ì ë˜ë©´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì¦ê°€í•©ë‹ˆë‹¤.

#### í•´ê²°: íˆìŠ¤í† ë¦¬ ì œí•œ
```python
# ìµœê·¼ Nê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€
if len(chat_history) > MAX_HISTORY_LENGTH:
    chat_history = chat_history[-MAX_HISTORY_LENGTH:]
```

### 2. ë„¤íŠ¸ì›Œí¬ ìµœì í™”

#### ì²­í¬ í¬ê¸° ì¡°ì ˆ
```python
# í•œ ê¸€ìì”© ì „ì†¡ (ë¶€ë“œëŸ¬ìš´ UX)
for char in delta:
    yield char
    await asyncio.sleep(0.01)

# ë˜ëŠ” ì—¬ëŸ¬ ê¸€ìì”© ì „ì†¡ (ë¹ ë¥¸ ì „ì†¡)
chunk_size = 5
for i in range(0, len(delta), chunk_size):
    chunk = delta[i:i + chunk_size]
    yield chunk
    await asyncio.sleep(0.01)
```

### 3. ë Œë”ë§ ìµœì í™”

#### React ìƒíƒœ ì—…ë°ì´íŠ¸ ìµœì í™”
```typescript
// âœ… ì¢‹ì€ ì˜ˆ: ë§ˆì§€ë§‰ ë©”ì‹œì§€ë§Œ ì—…ë°ì´íŠ¸
setMessages((prev) => {
  const updated = [...prev];
  const msgIndex = updated.findIndex((m) => m.id === streamingMessageId);
  if (msgIndex !== -1) {
    updated[msgIndex] = { ...updated[msgIndex], content: accumulatedText };
  }
  return updated;
});

// âŒ ë‚˜ìœ ì˜ˆ: ì „ì²´ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ì¬ìƒì„±
setMessages([...messages, { ...streamingMessage, content: accumulatedText }]);
```

---

## ë¬¸ì œ í•´ê²° ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: JSON ê°ì²´ê°€ í™”ë©´ì— í‘œì‹œë¨

#### ì¦ìƒ
```
{"delta":"ì•ˆë…•í•˜ì„¸ìš”"}
{"delta":" ì–´ë–»ê²Œ"}
{"delta":" ë„ì™€ë“œë¦´ê¹Œìš”?"}
```

#### ì›ì¸
Next.js API ë¼ìš°íŠ¸ì—ì„œ ë°±ì—”ë“œì˜ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¥¼ JSONìœ¼ë¡œ ê°ì‹¸ì„œ ì „ì†¡

#### í•´ê²°
```typescript
// âŒ ì˜ëª»ëœ ë°©ì‹
controller.enqueue(
  new TextEncoder().encode(JSON.stringify({ delta: chunk }) + '\n')
);

// âœ… ì˜¬ë°”ë¥¸ ë°©ì‹
return new Response(response.body, {
  headers: { "Content-Type": "text/plain; charset=utf-8" },
});
```

---

### ë¬¸ì œ 2: ì „ì²´ ë©”ì‹œì§€ê°€ ì¤‘ë³µ ì „ì†¡ë¨

#### ì¦ìƒ
ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì— ì´ì „ì— ë³´ë‚¸ í…ìŠ¤íŠ¸ê°€ ê³„ì† ë°˜ë³µë¨

#### ì›ì¸
`graph.stream()`ì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ìƒíƒœë¥¼ ë§¤ë²ˆ ì „ì†¡

#### í•´ê²°
```python
# âŒ ì˜ëª»ëœ ë°©ì‹
for event in graph.stream(state):
    messages = event.get("messages", [])
    yield messages[-1].content  # ì „ì²´ ë©”ì‹œì§€

# âœ… ì˜¬ë°”ë¥¸ ë°©ì‹
async for event in graph.astream_events(state, version="v1"):
    if event["event"] == "on_chat_model_stream":
        chunk = event["data"]["chunk"]
        yield chunk.content  # ìƒˆ í† í°ë§Œ
```

---

### ë¬¸ì œ 3: í•œê¸€ ë¬¸ìê°€ ê¹¨ì§

#### ì¦ìƒ
í•œê¸€ì´ `` ê°™ì€ ë¬¸ìë¡œ í‘œì‹œë¨

#### ì›ì¸
`TextDecoder`ì—ì„œ `{ stream: true }` ì˜µì…˜ ë¯¸ì‚¬ìš©

#### í•´ê²°
```typescript
// âœ… ì˜¬ë°”ë¥¸ ë°©ì‹
const decoder = new TextDecoder();
const chunk = decoder.decode(value, { stream: true });  // stream: true í•„ìˆ˜
```

---

### ë¬¸ì œ 4: ìŠ¤íŠ¸ë¦¬ë°ì´ ë„ˆë¬´ ë¹ ë¥´ê±°ë‚˜ ëŠë¦¼

#### ì¦ìƒ
- ë„ˆë¬´ ë¹ ë¦„: í™”ë©´ì´ ê¹œë¹¡ì„, CPU ì‚¬ìš©ëŸ‰ ì¦ê°€
- ë„ˆë¬´ ëŠë¦¼: ì‚¬ìš©ìê°€ ê¸°ë‹¤ë¦¼

#### í•´ê²°
```python
# ì§€ì—° ì‹œê°„ ì¡°ì ˆ
await asyncio.sleep(0.01)  # 10ms (ê¸°ë³¸ê°’)
await asyncio.sleep(0.005)  # 5ms (ë” ë¹ ë¦„)
await asyncio.sleep(0.02)   # 20ms (ë” ëŠë¦¼)
```

---

## ìš”ì•½

### í•µì‹¬ ì „ëµ
1. **ë°±ì—”ë“œ**: `astream_events` (LangGraph) ë˜ëŠ” `astream` (RAG Chain) ì‚¬ìš©
2. **í† í° ë‹¨ìœ„ ì „ì†¡**: ì „ì²´ ë©”ì‹œì§€ê°€ ì•„ë‹Œ ìƒˆë¡œ ìƒì„±ëœ í† í°ë§Œ yield
3. **ìˆœìˆ˜ í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°**: JSON ë³€í™˜ ì—†ì´ `text/plain`ìœ¼ë¡œ ì „ì†¡
4. **í”„ë¡ì‹œ íŒ¨í„´**: Next.js API ë¼ìš°íŠ¸ì—ì„œ ìŠ¤íŠ¸ë¦¼ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
5. **ì¦ë¶„ ì—…ë°ì´íŠ¸**: í”„ë¡ íŠ¸ì—”ë“œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ëˆ„ì í•˜ì—¬ ë§ˆì§€ë§‰ ë©”ì‹œì§€ë§Œ ì—…ë°ì´íŠ¸

### íŒŒì¼ êµ¬ì¡°
```
api/app/routers/
  â”œâ”€â”€ graph_router.py      # LangGraph ìŠ¤íŠ¸ë¦¬ë° (astream_events)
  â””â”€â”€ chat_router.py        # RAG Chain ìŠ¤íŠ¸ë¦¬ë° (astream)

frontend/app/api/
  â”œâ”€â”€ graph/route.ts        # LangGraph í”„ë¡ì‹œ
  â””â”€â”€ chat/route.ts         # RAG Chain í”„ë¡ì‹œ

frontend/app/
  â””â”€â”€ page.tsx              # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ë° UI ì—…ë°ì´íŠ¸
```

### ì„±ëŠ¥ ì§€í‘œ
- **ìŠ¤íŠ¸ë¦¬ë° ì§€ì—°**: ~10ms per token
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: íˆìŠ¤í† ë¦¬ ì œí•œìœ¼ë¡œ ìµœì í™”
- **ë„¤íŠ¸ì›Œí¬ íš¨ìœ¨**: ì¦ë¶„ ì—…ë°ì´íŠ¸ë¡œ ì¤‘ë³µ ì „ì†¡ ì œê±°

---

## ì°¸ê³  ìë£Œ

- [LangGraph Streaming Documentation](https://langchain-ai.github.io/langgraph/how-tos/streaming/)
- [FastAPI StreamingResponse](https://fastapi.tiangolo.com/advanced/custom-response/#streamingresponse)
- [Next.js API Routes](https://nextjs.org/docs/api-routes/introduction)
- [ReadableStream API](https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream)

---

**ì‘ì„±ì¼**: 2026-01-21
**ë²„ì „**: 1.0
**ì‘ì„±ì**: AI Assistant
