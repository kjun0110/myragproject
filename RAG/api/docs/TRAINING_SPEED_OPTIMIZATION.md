# í•™ìŠµ ì†ë„ ìµœì í™” ê°€ì´ë“œ

## í˜„ì¬ ìƒí™©

- **í˜„ì¬ ì†ë„**: 9.28ì´ˆ/it (ë§¤ìš° ëŠë¦¼)
- **ì´ ìŠ¤í…**: 1,228 ìŠ¤í…
- **ì˜ˆìƒ ì‹œê°„**: ì•½ 3.2ì‹œê°„

---

## í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬

### âœ… ì‚¬ìš© ì¤‘
1. **PEFT** (LoRA) - ì‚¬ìš© ì¤‘
2. **Transformers Trainer** - ì‚¬ìš© ì¤‘
3. **BitsAndBytes** (4-bit ì–‘ìí™”) - ì‚¬ìš© ì¤‘
4. **paged_adamw_8bit** (8-bit ì˜µí‹°ë§ˆì´ì €) - ì‚¬ìš© ì¤‘

### âŒ ì‚¬ìš© ì•ˆ í•¨
1. **xFormers** - ì‚¬ìš© ì•ˆ í•¨
2. **Flash Attention** - ì‚¬ìš© ì•ˆ í•¨
3. **Unsloth** - ì‚¬ìš© ì•ˆ í•¨
4. **torch.compile** - ì‚¬ìš© ì•ˆ í•¨

---

## âš ï¸ ì¤‘ìš”: Attention êµ¬í˜„ ë°©ì‹ ì„ íƒ

**xFormersì™€ Flash Attentionì€ ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒ ê°€ëŠ¥í•©ë‹ˆë‹¤!**

- ë‘˜ ë‹¤ attention ì—°ì‚°ì„ ìµœì í™”í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
- ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ
- í•˜ë‚˜ë¥¼ ì„ íƒí•˜ë©´ ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ì‚¬ìš© ë¶ˆê°€

### ì„ íƒ ê¸°ì¤€
1. **Unsloth ì‚¬ìš© ì‹œ**: Flash Attention ìë™ í¬í•¨ (xFormers ë¶ˆí•„ìš”)
2. **Unsloth ë¯¸ì‚¬ìš© ì‹œ**: xFormers ë˜ëŠ” Flash Attention ì¤‘ í•˜ë‚˜ ì„ íƒ
   - Windows: xFormers ê¶Œì¥ (ì„¤ì¹˜ ì‰¬ì›€)
   - Linux: Flash Attention ê¶Œì¥ (ë” ë¹ ë¦„)

---

## ì†ë„ ê°œì„  ë°©ë²• (ìš°ì„ ìˆœìœ„ ìˆœ)

### ğŸ¥‡ 1ìˆœìœ„: Unsloth ì‚¬ìš© (ê°€ì¥ íš¨ê³¼ì )

**íš¨ê³¼**: 2-5ë°° ì†ë„ í–¥ìƒ ê°€ëŠ¥

**ì¥ì :**
- PEFT + Transformers ëŒ€ë¹„ í›¨ì”¬ ë¹ ë¦„
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- LoRA í•™ìŠµì— ìµœì í™”
- **Flash Attention ìë™ í¬í•¨** (xFormers ë¶ˆí•„ìš”)

**âš ï¸ ì¤‘ìš”:**
- Unslothë¥¼ ì‚¬ìš©í•˜ë©´ **ìë™ìœ¼ë¡œ Flash Attentionì´ í™œì„±í™”**ë¨
- xFormersë¥¼ ë³„ë„ë¡œ ì„¤ì •í•  í•„ìš” ì—†ìŒ
- Unslothê°€ í˜¸í™˜ë˜ì§€ ì•Šìœ¼ë©´ â†’ xFormers ì‚¬ìš©

**ì„¤ì¹˜:**
```bash
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# ë˜ëŠ”
pip install unsloth
```

**ì‚¬ìš© ë°©ë²•:**
```python
from unsloth import FastLanguageModel

# ëª¨ë¸ ë¡œë“œ (Unsloth ì‚¬ìš©)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct",
    max_seq_length=256,
    dtype=None,  # ìë™ ê°ì§€
    load_in_4bit=True,  # 4-bit ì–‘ìí™”
    trust_remote_code=True,
)

# LoRA ì„¤ì •
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    use_gradient_checkpointing=True,
    random_state=3407,
)

# í•™ìŠµ (Unsloth Trainer ì‚¬ìš©)
from unsloth import FastSFTTrainer

trainer = FastSFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    max_seq_length=256,
    dataset_text_field="text",
    packing=False,
    args=TrainingArguments(
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        warmup_steps=100,
        num_train_epochs=1,
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=10,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="outputs",
    ),
)
```

**ì˜ˆìƒ ì†ë„ ê°œì„ :**
- í˜„ì¬: 9.28ì´ˆ/it
- Unsloth ì‚¬ìš©: **2-4ì´ˆ/it** (ì•½ 2-4ë°° ë¹ ë¦„)

---

### ğŸ¥ˆ 2ìˆœìœ„: Flash Attention ì‚¬ìš© (Unsloth ë¯¸ì‚¬ìš© ì‹œ)

**íš¨ê³¼**: 1.5-2ë°° ì†ë„ í–¥ìƒ ê°€ëŠ¥

**âš ï¸ ì¤‘ìš”:**
- **Unslothë¥¼ ì‚¬ìš©í•˜ë©´ Flash Attentionì´ ìë™ í¬í•¨ë˜ë¯€ë¡œ ë³„ë„ ì„¤ì • ë¶ˆí•„ìš”**
- Unslothë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œë§Œ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •

**ì„¤ì¹˜:**
```bash
pip install flash-attn --no-build-isolation
# ë˜ëŠ”
pip install flash-attn
```

**ì‚¬ìš© ë°©ë²•:**
```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    attn_implementation="flash_attention_2",  # Flash Attention ì‚¬ìš©
    # ... ê¸°íƒ€ ì˜µì…˜
)
```

**ì£¼ì˜ì‚¬í•­:**
- CUDA 11.8+ í•„ìš”
- Windowsì—ì„œëŠ” ì„¤ì¹˜ê°€ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ
- **xFormersì™€ ë™ì‹œ ì‚¬ìš© ë¶ˆê°€** (ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒ)

---

### ğŸ¥‰ 3ìˆœìœ„: torch.compile ì‚¬ìš©

**íš¨ê³¼**: 1.2-1.5ë°° ì†ë„ í–¥ìƒ ê°€ëŠ¥

**ì‚¬ìš© ë°©ë²•:**
```python
import torch

# ëª¨ë¸ ì»´íŒŒì¼
model = torch.compile(model, mode="reduce-overhead")

# ë˜ëŠ” ë” ê³µê²©ì ì¸ ìµœì í™”
model = torch.compile(model, mode="max-autotune")
```

**ì£¼ì˜ì‚¬í•­:**
- PyTorch 2.0+ í•„ìš”
- ì²« ì‹¤í–‰ ì‹œ ì»´íŒŒì¼ ì‹œê°„ ì†Œìš” (ëŠë¦¼)
- ì´í›„ ì‹¤í–‰ ì‹œ ë¹ ë¦„

---

### 4ìˆœìœ„: xFormers ì‚¬ìš© âš ï¸ ì¤‘ìš” (Unsloth ë¯¸ì‚¬ìš© ì‹œ)

**íš¨ê³¼**: 1.2-2ë°° ì†ë„ í–¥ìƒ ê°€ëŠ¥ (Flash Attentionê³¼ ìœ ì‚¬)

**âš ï¸ ì¤‘ìš”:**
- **Unslothë¥¼ ì‚¬ìš©í•˜ë©´ Flash Attentionì´ ìë™ í¬í•¨ë˜ë¯€ë¡œ xFormers ë¶ˆí•„ìš”**
- Unslothë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œë§Œ xFormers ì‚¬ìš©
- **Flash Attentionê³¼ ë™ì‹œ ì‚¬ìš© ë¶ˆê°€** (ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒ)

**xFormersë€?**
- Facebook(Meta)ì—ì„œ ê°œë°œí•œ íš¨ìœ¨ì ì¸ attention êµ¬í˜„
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ê³  ë¹ ë¥¸ attention ì—°ì‚°
- Flash Attentionê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥
- Windowsì—ì„œ ì„¤ì¹˜ê°€ ë” ì‰¬ì›€

**ì„¤ì¹˜:**
```bash
# Windows (ê¶Œì¥)
pip install xformers

# ë˜ëŠ” íŠ¹ì • ë²„ì „
pip install xformers==0.0.23.post1
```

**ì‚¬ìš© ë°©ë²•:**
```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    attn_implementation="xformers",  # xFormers ì‚¬ìš©
    # ... ê¸°íƒ€ ì˜µì…˜
)
```

**í˜„ì¬ ì½”ë“œ ì ìš© ìœ„ì¹˜:**
- `api/app/common/loaders/model_loader.py`ì˜ `load_exaone_model()` í•¨ìˆ˜
- `api/training/agents/spam_agent/load_model.py`ì˜ `load_exaone_model()` í•¨ìˆ˜

**ì£¼ì˜ì‚¬í•­:**
- Windowsì—ì„œë„ ì„¤ì¹˜ ê°€ëŠ¥ (ìµœì‹  ë²„ì „)
- Flash Attentionë³´ë‹¤ ì„¤ì¹˜ê°€ ì‰¬ì›€
- CUDA 11.8+ ê¶Œì¥
- ì¼ë¶€ ëª¨ë¸ì—ì„œëŠ” Flash Attentionì´ ë” ë¹ ë¥¼ ìˆ˜ ìˆìŒ

**xFormers vs Flash Attention:**
| í•­ëª© | xFormers | Flash Attention |
|-----|---------|----------------|
| **ì†ë„** | ë¹ ë¦„ (1.2-2ë°°) | ë§¤ìš° ë¹ ë¦„ (1.5-2.5ë°°) |
| **ì„¤ì¹˜** | ì‰¬ì›€ (Windows ì§€ì›) | ì–´ë ¤ì›€ (Windows ì–´ë ¤ì›€) |
| **ë©”ëª¨ë¦¬** | íš¨ìœ¨ì  | ë§¤ìš° íš¨ìœ¨ì  |
| **í˜¸í™˜ì„±** | ë†’ìŒ | ì¤‘ê°„ |

---

## ê¶Œì¥ ì¡°í•©

### ìµœì  ì¡°í•© 1: Unsloth ì‚¬ìš© (ê°€ì¥ ë¹ ë¦„)
1. **Unsloth** (í•„ìˆ˜) - 2-5ë°° í–¥ìƒ
2. **torch.compile** (ì„ íƒ) - ì¶”ê°€ 1.2-1.5ë°° í–¥ìƒ

**ì˜ˆìƒ ì†ë„:**
- í˜„ì¬: 9.28ì´ˆ/it
- ìµœì í™” í›„: **1.5-3ì´ˆ/it** (ì•½ 3-6ë°° ë¹ ë¦„)

### ìµœì  ì¡°í•© 2: xFormers ì‚¬ìš© (Unsloth ëŒ€ì•ˆ)
1. **xFormers** (í•„ìˆ˜) - 1.2-2ë°° í–¥ìƒ
2. **torch.compile** (ì„ íƒ) - ì¶”ê°€ 1.2-1.5ë°° í–¥ìƒ

**ì˜ˆìƒ ì†ë„:**
- í˜„ì¬: 9.28ì´ˆ/it
- ìµœì í™” í›„: **3-5ì´ˆ/it** (ì•½ 2-3ë°° ë¹ ë¦„)

### ìµœì  ì¡°í•© 3: Flash Attention ì‚¬ìš© (ê°€ì¥ ë¹ ë¥´ì§€ë§Œ ì„¤ì¹˜ ì–´ë ¤ì›€)
1. **Flash Attention** (í•„ìˆ˜) - 1.5-2.5ë°° í–¥ìƒ
2. **torch.compile** (ì„ íƒ) - ì¶”ê°€ 1.2-1.5ë°° í–¥ìƒ

**ì˜ˆìƒ ì†ë„:**
- í˜„ì¬: 9.28ì´ˆ/it
- ìµœì í™” í›„: **2-4ì´ˆ/it** (ì•½ 2.5-4.5ë°° ë¹ ë¦„)

---

## Unsloth vs í˜„ì¬ ë°©ì‹ ë¹„êµ

| í•­ëª© | í˜„ì¬ (PEFT + Trainer) | Unsloth |
|-----|---------------------|---------|
| **ì†ë„** | 9.28ì´ˆ/it | 2-4ì´ˆ/it |
| **ë©”ëª¨ë¦¬** | í‘œì¤€ | ë” íš¨ìœ¨ì  |
| **ì„¤ì¹˜** | ê°„ë‹¨ | ê°„ë‹¨ |
| **í˜¸í™˜ì„±** | ë†’ìŒ | ë†’ìŒ |
| **Flash Attention** | ìˆ˜ë™ ì„¤ì • | ìë™ í¬í•¨ |
| **ìµœì í™”** | ìˆ˜ë™ | ìë™ |

---

## Unsloth ì‚¬ìš© ì‹œ ì£¼ì˜ì‚¬í•­

### 1. ëª¨ë¸ í˜¸í™˜ì„±
- EXAONE ëª¨ë¸ì´ Unslothë¥¼ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸ í•„ìš”
- `trust_remote_code=True` í•„ìš”í•  ìˆ˜ ìˆìŒ

### 2. ì»¤ìŠ¤í…€ ëª¨ë¸ë§ ì½”ë“œ
- EXAONEì€ ì»¤ìŠ¤í…€ `modeling_exaone.py` ì‚¬ìš©
- Unslothê°€ ì´ë¥¼ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸ í•„ìš”

### 3. Windows í˜¸í™˜ì„±
- UnslothëŠ” Linux/Colabì—ì„œ ë” ì˜ ì‘ë™
- Windowsì—ì„œë„ ì‘ë™í•˜ì§€ë§Œ ì¼ë¶€ ìµœì í™”ê°€ ì œí•œë  ìˆ˜ ìˆìŒ

---

## ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ìµœì í™” (ì½”ë“œ ìˆ˜ì • ì—†ì´)

### 1. ë°°ì¹˜ í¬ê¸° ì¦ê°€
- í˜„ì¬: `per_device_train_batch_size=4`
- ê¶Œì¥: ë©”ëª¨ë¦¬ í—ˆìš© ì‹œ 8-16ìœ¼ë¡œ ì¦ê°€
- íš¨ê³¼: GPU í™œìš©ë¥  ì¦ê°€ â†’ ì†ë„ í–¥ìƒ

### 2. ì‹œí€€ìŠ¤ ê¸¸ì´ ê°ì†Œ
- í˜„ì¬: `max_seq_length=256`
- ì´ë¯¸ ìµœì í™”ë¨ (512 â†’ 256)

### 3. Gradient Accumulation ê°ì†Œ
- í˜„ì¬: `gradient_accumulation_steps=4`
- ë°°ì¹˜ í¬ê¸° ì¦ê°€ ì‹œ 2ë¡œ ê°ì†Œ ê°€ëŠ¥
- íš¨ê³¼: ì—…ë°ì´íŠ¸ ë¹ˆë„ ì¦ê°€ â†’ ì•½ê°„ ë¹ ë¦„

---

## ë‹¨ê³„ë³„ ì ìš© ê³„íš

### Phase 1: ì¦‰ì‹œ ì ìš© (ì½”ë“œ ìˆ˜ì • ìµœì†Œ)
1. ë°°ì¹˜ í¬ê¸° ì¦ê°€ (ë©”ëª¨ë¦¬ í™•ì¸ í›„)
2. Gradient Accumulation ê°ì†Œ

**ì˜ˆìƒ ê°œì„ **: 10-20% ì†ë„ í–¥ìƒ

### Phase 2: Unsloth ë„ì… (ê¶Œì¥)
1. Unsloth ì„¤ì¹˜
2. ì½”ë“œ ìˆ˜ì • (ëª¨ë¸ ë¡œë“œ ë¶€ë¶„)
3. Trainerë¥¼ FastSFTTrainerë¡œ ë³€ê²½

**ì˜ˆìƒ ê°œì„ **: 2-4ë°° ì†ë„ í–¥ìƒ

### Phase 3: ì¶”ê°€ ìµœì í™”
1. torch.compile ì ìš©
2. Flash Attention í™•ì¸ (Unslothì— í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ)

**ì˜ˆìƒ ê°œì„ **: ì¶”ê°€ 20-50% ì†ë„ í–¥ìƒ

---

## ì˜ˆìƒ ìµœì¢… ì†ë„

### í˜„ì¬
- ì†ë„: 9.28ì´ˆ/it
- ì´ ì‹œê°„: ì•½ 3.2ì‹œê°„

### Phase 1 ì ìš© í›„
- ì†ë„: ì•½ 8ì´ˆ/it
- ì´ ì‹œê°„: ì•½ 2.7ì‹œê°„

### Phase 2 ì ìš© í›„

#### xFormers ì‚¬ìš© ì‹œ
- ì†ë„: ì•½ 4-7ì´ˆ/it
- ì´ ì‹œê°„: ì•½ 1.4-2.4ì‹œê°„

#### Unsloth ì‚¬ìš© ì‹œ
- ì†ë„: ì•½ 2-4ì´ˆ/it
- ì´ ì‹œê°„: ì•½ 0.7-1.4ì‹œê°„

### Phase 3 ì ìš© í›„ (ì „ì²´ ìµœì í™”)
- ì†ë„: ì•½ 1.5-3ì´ˆ/it
- ì´ ì‹œê°„: ì•½ 0.5-1ì‹œê°„

---

## ì°¸ê³  ìë£Œ

- [Unsloth ê³µì‹ ë¬¸ì„œ](https://github.com/unslothai/unsloth)
- [Flash Attention ë¬¸ì„œ](https://github.com/Dao-AILab/flash-attention)
- [torch.compile ë¬¸ì„œ](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
