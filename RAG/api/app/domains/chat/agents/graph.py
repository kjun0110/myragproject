# âœ… ë¡œì»¬ Exaone3.5 ëª¨ë¸ ì‚¬ìš© (model_serviceë¥¼ í†µí•´ ë¡œë“œ)
import re
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.tools import tool
from langgraph.graph import END, StateGraph

# State ì •ì˜ import
from app.domains.chat.models.state_model import AgentState


# -------------------------
# 2) Tool ì •ì˜(ì˜ˆì‹œ)
# -------------------------
@tool
def get_server_time() -> str:
    """Return server time as ISO string."""
    from datetime import datetime, timezone

    return datetime.now(timezone.utc).isoformat()


TOOLS = [get_server_time]

# -------------------------
# 3) Model ë…¸ë“œ - ë¡œì»¬ Exaone3.5 ëª¨ë¸ ë¡œë“œ
# -------------------------
# ëª¨ë¸ì€ ì„œë²„ ì‹œì‘ ì‹œ ë¯¸ë¦¬ ë¡œë“œ (eager loading)
_llm = None
_llm_loading = False
_llm_error = None


def _load_exaone_model():
    """Exaone3.5 ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜."""
    global _llm, _llm_loading, _llm_error

    if _llm is not None:
        return _llm

    # ì´ì „ ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì¬ì‹œë„í•˜ì§€ ì•Šê³  ë°”ë¡œ ì—ëŸ¬ ë°˜í™˜
    if _llm_error is not None:
        raise RuntimeError(f"ì´ì „ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {_llm_error}")

    if _llm_loading:
        raise RuntimeError("ëª¨ë¸ì´ í˜„ì¬ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

    _llm_loading = True
    try:
        # chat/agentsì—ì„œ import
        from .model_loader import load_exaone_model_for_service

        # Exaone ëª¨ë¸ ë¡œë“œ
        print("[INFO] Exaone3.5 ëª¨ë¸ ë¡œë”© ì‹œì‘... (ì´ ì‘ì—…ì€ ëª‡ ë¶„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        _llm = load_exaone_model_for_service()

        # Tool binding
        _llm = _llm.bind_tools(TOOLS)
        print("[OK] ë¡œì»¬ Exaone3.5 ëª¨ë¸ì´ graph.pyì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("[INFO] Tool callingì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        _llm_loading = False
        _llm_error = None  # ì„±ê³µ ì‹œ ì—ëŸ¬ ìƒíƒœ ì´ˆê¸°í™”
        return _llm

    except Exception as e:
        _llm_loading = False
        error_msg = f"Exaone3.5 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        _llm_error = error_msg
        print(f"[ERROR] {error_msg}")
        import traceback

        print(f"[ERROR] ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
        raise RuntimeError(error_msg) from e


def preload_exaone_model():
    """ì„œë²„ ì‹œì‘ ì‹œ Exaone ëª¨ë¸ì„ ë¯¸ë¦¬ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜."""
    global _llm
    if _llm is None:
        print("[INFO] Exaone3.5 ëª¨ë¸ì„ ë¯¸ë¦¬ ë¡œë“œí•©ë‹ˆë‹¤...")
        try:
            _load_exaone_model()
            print("[OK] Exaone3.5 ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"[WARNING] Exaone3.5 ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            print("[INFO] ì²« ìš”ì²­ ì‹œ ë¡œë“œë©ë‹ˆë‹¤.")


def model_node(state: AgentState):
    # state["messages"]ëŠ” ëˆ„ì  ë©”ì‹œì§€
    # Lazy loading: ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì§€ê¸ˆ ë¡œë“œ
    llm_with_tools = _load_exaone_model()
    resp = llm_with_tools.invoke(state["messages"])
    return {"messages": [resp]}


# -------------------------
# 4) Tool ë…¸ë“œ
# -------------------------
def tool_node(state: AgentState):
    # ê°€ì¥ ìµœê·¼ AIMessageê°€ tool_callsë¥¼ ê°–ëŠ”ì§€ í™•ì¸
    last = state["messages"][-1]
    tool_calls = getattr(last, "tool_calls", None) or []

    results = []
    # tool_callsëŠ” {"name": ..., "args": ...} í˜•íƒœë¥¼ í¬í•¨
    tool_map = {t.name: t for t in TOOLS}

    for call in tool_calls:
        name = call["name"]
        args = call.get("args") or {}
        if name not in tool_map:
            results.append(f"Tool {name} not found")
            continue
        out = tool_map[name].invoke(args)
        # Tool ê²°ê³¼ëŠ” ToolMessageë¡œ ë„£ëŠ” ê²Œ ì •ì„ì´ì§€ë§Œ,
        # LangChainì´ ë°˜í™˜ íƒ€ì…ì„ ToolMessageë¡œ ê°ì‹¸ì£¼ëŠ” ê²½ë¡œê°€ ìˆì–´
        # ê°„ë‹¨íˆ ë¬¸ìì—´ë¡œë„ ë™ì‘í•©ë‹ˆë‹¤(í…ŒìŠ¤íŠ¸ ëª©ì ).
        results.append(out)

    # Tool ê²°ê³¼ë¥¼ ë‹¤ìŒ ëª¨ë¸ ì…ë ¥ ë©”ì‹œì§€ë¡œ ì—°ê²°
    from langchain_core.messages import ToolMessage

    tool_messages = []
    for i, out in enumerate(results):
        tool_messages.append(
            ToolMessage(content=str(out), tool_call_id=tool_calls[i]["id"])
        )
    return {"messages": tool_messages}


# -------------------------
# 5) ì¡°ê±´ ë¶„ê¸°(ë„êµ¬ í˜¸ì¶œ ì—¬ë¶€)
# -------------------------
def should_use_tools(state: AgentState):
    last = state["messages"][-1]
    tool_calls = getattr(last, "tool_calls", None) or []
    if tool_calls:
        return "tools"
    return "end"


# -------------------------
# 6) Graph ë¹Œë“œ
# -------------------------
def build_graph():
    g = StateGraph(AgentState)
    g.add_node("model", model_node)
    g.add_node("tools", tool_node)

    g.set_entry_point("model")
    g.add_conditional_edges(
        "model",
        should_use_tools,
        {
            "tools": "tools",
            "end": END,
        },
    )
    g.add_edge("tools", "model")
    return g.compile()


graph = build_graph()


# -------------------------
# 7) ê°„ë‹¨ ì‹¤í–‰ í—¬í¼
# -------------------------
def run_once(user_text: str):
    init_state: AgentState = {
        "messages": [
            SystemMessage(
                content="You are a helpful assistant. Use tools when needed."
            ),
            HumanMessage(content=user_text),
        ]
    }
    out = graph.invoke(init_state)
    response_text = out["messages"][-1].content

    # ì‘ë‹µ ì •ë¦¬: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, íƒœê·¸, ê³¼ê±° ëŒ€í™” ë‚´ìš© ì œê±°
    if response_text:
        # 1. [[system]], [[endofturn]], [[assistant]] ê°™ì€ íƒœê·¸ ì œê±°
        response_text = re.sub(r'\[\[system\]\].*?\[\[endofturn\]\]\s*', '', response_text, flags=re.DOTALL)
        response_text = re.sub(r'\[\[assistant\]\]\s*', '', response_text, flags=re.IGNORECASE)
        response_text = re.sub(r'\[\[endofturn\]\]\s*', '', response_text, flags=re.IGNORECASE)
        response_text = re.sub(r'\[\[user\]\]\s*', '', response_text, flags=re.IGNORECASE)

        # 2. Human:, Assistant: ê°™ì€ ì´ì „ ëŒ€í™” í˜•ì‹ ì œê±°
        if "Human:" in response_text or "Assistant:" in response_text:
            # ë§ˆì§€ë§‰ Assistant: ì´í›„ë§Œ ì¶”ì¶œ
            assistant_match = re.search(
                r"Assistant:\s*(.+?)(?:\nHuman:|$)", response_text, re.DOTALL
            )
            if assistant_match:
                response_text = assistant_match.group(1).strip()

        # 3. ê°„ë‹¨í•œ ì¸ì‚¬ì— ëŒ€í•œ ì‘ë‹µ ì •ë¦¬
        if any(greeting in user_text.lower() for greeting in ["ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "hi", "hello"]):
            # ì¸ì‚¬ì— ëŒ€í•œ ê°„ë‹¨í•œ ì‘ë‹µë§Œ ë‚¨ê¸°ê¸°
            lines = response_text.split('\n')
            clean_lines = []
            for line in lines:
                line = line.strip()
                # ë¹ˆ ì¤„, íƒœê·¸, ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸
                if not line or line.startswith('[') or 'system' in line.lower() or 'endofturn' in line.lower():
                    continue
                # ì‚¬ìš©ì ì§ˆë¬¸ ë°˜ë³µ ì œê±°
                if any(greeting in line.lower() for greeting in ["ë„ˆ ì´ë¦„ì´", "what's your name", "who are you"]):
                    continue
                clean_lines.append(line)

            if clean_lines:
                response_text = '\n'.join(clean_lines)
            else:
                # ê¸°ë³¸ ì¸ì‚¬ ì‘ë‹µ
                response_text = "ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ ìˆ˜ ìˆì„ê¹Œìš”? ê¶ê¸ˆí•œ ì ì´ ìˆê±°ë‚˜ ë„ì›€ì´ í•„ìš”í•œ ì‚¬í•­ì´ ìˆìœ¼ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ğŸ˜Š"

    return response_text
