"""
FastAPI ë°±ì—”ë“œ ì„œë²„ - ì±„íŒ… ì„œë¹„ìŠ¤.

ì±„íŒ… ê´€ë ¨ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” API ì„œë²„ì…ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path

# .env íŒŒì¼ ë¡œë“œ
try:
    from dotenv import load_dotenv

    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
    current_file = Path(__file__).resolve()
    app_dir = current_file.parent  # api/app/
    api_dir = app_dir.parent  # api/
    project_root = api_dir.parent  # í”„ë¡œì íŠ¸ ë£¨íŠ¸

    env_file = project_root / ".env"
    if env_file.exists():
        load_dotenv(env_file)
    else:
        load_dotenv()
except ImportError:
    pass

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(api_dir))

# ê³µí†µ ëª¨ë“ˆ import
from app.common.database.vector_store import (
    COLLECTION_NAME,
    CONNECTION_STRING,
    wait_for_postgres,
)
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Chat Service API",
    description="ì±„íŒ… ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” API",
    version="1.0.0",
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ìš´ì˜ í™˜ê²½ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜: ChatService ì¸ìŠ¤í„´ìŠ¤
chat_service = None


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™” ì‘ì—…."""
    global chat_service

    print("\n" + "=" * 60)
    print("ğŸš€ ì±„íŒ… ì„œë¹„ìŠ¤ ì„œë²„ ì´ˆê¸°í™” ì‹œì‘")
    print("=" * 60)

    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    llm_provider = os.getenv("LLM_PROVIDER", "openai")
    local_model_dir = os.getenv("LOCAL_MODEL_DIR", "ê¸°ë³¸ê°’ ì‚¬ìš©")
    print(f"\n[INFO] LLM_PROVIDER: {llm_provider}")
    print(f"[INFO] LOCAL_MODEL_DIR: {local_model_dir}")

    # 1. Neon PostgreSQL ì—°ê²° ëŒ€ê¸°
    print("\n1. Neon PostgreSQL ì—°ê²° í™•ì¸ ì¤‘...")
    wait_for_postgres()

    # 2. ChatService ì´ˆê¸°í™”
    print("\n2. ChatService ì´ˆê¸°í™” ì¤‘...")
    from app.domains.chat.agents.chat_service import ChatService

    chat_service = ChatService(
        connection_string=CONNECTION_STRING,
        collection_name=COLLECTION_NAME,
        model_name_or_path=local_model_dir
        if local_model_dir != "ê¸°ë³¸ê°’ ì‚¬ìš©"
        else None,
    )

    # 3. Embedding ëª¨ë¸ ì´ˆê¸°í™”
    print("\n3. Embedding ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    chat_service.initialize_embeddings()

    # 4. LLM ëª¨ë¸ ì´ˆê¸°í™”
    print("\n4. LLM ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    chat_service.initialize_llm()

    # 5. PGVector ìŠ¤í† ì–´ ì´ˆê¸°í™”
    print("\n5. PGVector ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘...")
    from app.common.database.vector_store import initialize_vector_store

    initialize_vector_store()

    # 6. RAG ì²´ì¸ ì´ˆê¸°í™”
    print("\n6. RAG ì²´ì¸ ì´ˆê¸°í™” ì¤‘...")
    chat_service.initialize_rag_chain()

    # 7. Exaone ëª¨ë¸ ì‚¬ì „ ë¡œë“œ (LangGraphìš©)
    print("\n7. Exaone3.5 ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì¤‘...")
    try:
        from app.domains.chat.agents.graph import preload_exaone_model

        preload_exaone_model()
    except Exception as e:
        print(f"[WARNING] Exaone ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        print("[INFO] ì²« ìš”ì²­ ì‹œ ë¡œë“œë©ë‹ˆë‹¤ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤).")

    print("\n" + "=" * 60)
    print("[OK] ì±„íŒ… ì„œë¹„ìŠ¤ ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ!")
    print("=" * 60)


# ë¼ìš°í„° ë“±ë¡
from app.routers.chat_router import router as chat_router
from app.routers.graph_router import router as graph_router

app.include_router(chat_router)
app.include_router(graph_router)


@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸."""
    return {
        "message": "Chat Service API",
        "status": "running",
        "docs": "/docs",
    }


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸."""
    global chat_service
    if chat_service is None:
        return {
            "status": "initializing",
            "chat_service": "not initialized",
        }

    return {
        "status": "healthy",
        "chat_service": "initialized",
        "openai_embeddings": "initialized"
        if chat_service.openai_embeddings
        else "not initialized",
        "local_embeddings": "initialized"
        if chat_service.local_embeddings
        else "not initialized",
        "openai_llm": "initialized" if chat_service.openai_llm else "not initialized",
        "local_llm": "initialized" if chat_service.local_llm else "not initialized",
        "openai_rag_chain": "initialized"
        if chat_service.openai_rag_chain
        else "not initialized",
        "local_rag_chain": "initialized"
        if chat_service.local_rag_chain
        else "not initialized",
        "openai_quota_exceeded": chat_service.openai_quota_exceeded,
    }


if __name__ == "__main__":
    import uvicorn

    # í¬íŠ¸ ì„¤ì •
    port = int(os.getenv("PORT", 8000))
    host = os.getenv("HOST", "0.0.0.0")

    print("=" * 60)
    print("ğŸš€ ì±„íŒ… ì„œë¹„ìŠ¤ ì„œë²„ ì‹œì‘")
    print("=" * 60)
    print(f"ğŸ“ ì„œë²„ ì£¼ì†Œ: http://{host}:{port}")
    print(f"ğŸ“š API ë¬¸ì„œ: http://{host}:{port}/docs")
    print(f"ğŸ” í—¬ìŠ¤ ì²´í¬: http://{host}:{port}/health")
    print("=" * 60)
    print("\nì£¼ìš” ì—”ë“œí¬ì¸íŠ¸:")
    print("  - POST /api/chain    : ì±„íŒ… API (RAG ì²´ì¸)")
    print("  - POST /api/graph    : ê·¸ë˜í”„ API (LangGraph)")
    print("=" * 60)
    print()

    uvicorn.run(
        "app.mainbackup:app",
        host=host,
        port=port,
        reload=True,
        log_level="info",
    )
