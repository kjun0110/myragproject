"""
ğŸ˜ğŸ˜ chat_service_t.py ì„œë¹™ ê´€ë ¨ ì„œë¹„ìŠ¤

PEFT QLoRA ë°©ì‹ìœ¼ë¡œ ëŒ€í™”í•˜ê³  í•™ìŠµí•˜ëŠ” ê¸°ëŠ¥ í¬í•¨.

ì„¸ì…˜ë³„ íˆìŠ¤í† ë¦¬ ê´€ë¦¬, ìš”ì•½, í† í° ì ˆì•½ ì „ëµ ë“±.
"""

import json
import os
import re
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import torch
from datasets import Dataset
from langchain_classic.chains import (
    create_history_aware_retriever,
    create_retrieval_chain,
)
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_community.vectorstores import PGVector
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import Runnable
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from peft import (
    LoraConfig,
    PeftModel,
    TaskType,
    get_peft_model,
    prepare_model_for_kbit_training,
)
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    DataCollatorForLanguageModeling,
    TrainingArguments,
)

try:
    from trl import SFTTrainer
except ImportError:
    from trl.trainer.sft_trainer import SFTTrainer

try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings


class ChatService:
    """ì±„íŒ… ì„œë¹„ìŠ¤ - ëª¨ë¸ ë¡œë”© ë° RAG ì²´ì¸ ê´€ë¦¬."""

    def __init__(
        self,
        connection_string: str,
        collection_name: str,
        model_name_or_path: Optional[str] = None,
    ):
        """ChatService ì´ˆê¸°í™”.

        Args:
            connection_string: PostgreSQL ì—°ê²° ë¬¸ìì—´
            collection_name: PGVector ì»¬ë ‰ì…˜ ì´ë¦„
            model_name_or_path: ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        """
        self.connection_string = connection_string
        self.collection_name = collection_name
        self.model_name_or_path = model_name_or_path

        # ëª¨ë¸ ë° ì²´ì¸
        self.openai_embeddings: Optional[OpenAIEmbeddings] = None
        self.local_embeddings: Optional[HuggingFaceEmbeddings] = None
        self.openai_llm: Optional[ChatOpenAI] = None
        self.local_llm: Optional[Any] = None
        self.openai_rag_chain: Optional[Runnable] = None
        self.local_rag_chain: Optional[Runnable] = None
        self.openai_quota_exceeded = False
        self.vector_store: Optional[PGVector] = None

    def initialize_embeddings(self) -> None:
        """Embedding ëª¨ë¸ ì´ˆê¸°í™” - OpenAIì™€ ë¡œì»¬ ëª¨ë¸ ëª¨ë‘ ì´ˆê¸°í™”."""
        openai_api_key = os.getenv("OPENAI_API_KEY")

        # OpenAI Embedding ì´ˆê¸°í™”
        if openai_api_key and openai_api_key != "your-api-key-here":
            try:
                self.openai_embeddings = OpenAIEmbeddings()
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
                self.openai_embeddings.embed_query("test")
                print("[OK] OpenAI Embedding ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                error_msg = str(e)
                if (
                    "quota" in error_msg.lower()
                    or "429" in error_msg
                    or "insufficient_quota" in error_msg
                ):
                    self.openai_quota_exceeded = True
                    print(f"[WARNING] OpenAI API í• ë‹¹ëŸ‰ ì´ˆê³¼: {error_msg[:100]}...")
                    print("   OpenAI Embeddingì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    self.openai_embeddings = None
                else:
                    print(
                        f"[WARNING] OpenAI Embedding ì´ˆê¸°í™” ì‹¤íŒ¨: {error_msg[:100]}..."
                    )
                    self.openai_embeddings = None
        else:
            print("[WARNING] OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.openai_embeddings = None

        # ë¡œì»¬ Embedding ì´ˆê¸°í™”
        try:
            embedding_device = os.getenv("EMBEDDING_DEVICE", "cpu")
            self.local_embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                model_kwargs={"device": embedding_device},
            )
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
            self.local_embeddings.embed_query("test")
            print(
                f"[OK] ë¡œì»¬ Embedding ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (sentence-transformers, device={embedding_device})"
            )
        except Exception as local_error:
            print(
                f"[WARNING] ë¡œì»¬ Embedding ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(local_error)[:100]}..."
            )
            self.local_embeddings = None

        if not self.openai_embeddings and not self.local_embeddings:
            raise RuntimeError(
                "OpenAIì™€ ë¡œì»¬ Embedding ëª¨ë¸ ëª¨ë‘ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. "
                "OpenAI API í‚¤ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ sentence-transformersë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”."
            )

    def initialize_llm(self) -> None:
        """LLM ëª¨ë¸ ì´ˆê¸°í™” - OpenAIì™€ ë¡œì»¬ ëª¨ë¸ ëª¨ë‘ ì´ˆê¸°í™”."""
        openai_api_key = os.getenv("OPENAI_API_KEY")

        # OpenAI LLM ì´ˆê¸°í™”
        if openai_api_key and openai_api_key != "your-api-key-here":
            try:
                self.openai_llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
                # ì‹¤ì œ API í˜¸ì¶œ í…ŒìŠ¤íŠ¸ (í• ë‹¹ëŸ‰ í™•ì¸)
                self.openai_llm.invoke("test")
                print("[OK] OpenAI Chat ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                error_msg = str(e)
                if (
                    "quota" in error_msg.lower()
                    or "429" in error_msg
                    or "insufficient_quota" in error_msg
                ):
                    self.openai_quota_exceeded = True
                    print(f"[WARNING] OpenAI API í• ë‹¹ëŸ‰ ì´ˆê³¼: {error_msg[:100]}...")
                    print("   OpenAI LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    self.openai_llm = None
                else:
                    print(
                        f"[WARNING] OpenAI Chat ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {error_msg[:100]}..."
                    )
                    self.openai_llm = None
        else:
            print("[WARNING] OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.openai_llm = None

        # ë¡œì»¬ Midm LLM ì´ˆê¸°í™”
        try:
            from app.model.model_loader import load_midm_model

            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            # .env íŒŒì¼ì—ì„œ LOCAL_MODEL_DIR ì½ê¸°
            local_model_dir = self.model_name_or_path or os.getenv("LOCAL_MODEL_DIR")
            if local_model_dir:
                # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if not Path(local_model_dir).is_absolute():
                    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
                    project_root = Path(__file__).parent.parent.parent.parent
                    local_model_dir = str(project_root / local_model_dir)
                print(f"[INFO] ë¡œì»¬ ëª¨ë¸ ë””ë ‰í† ë¦¬: {local_model_dir}")
                midm_model = load_midm_model(
                    model_path=local_model_dir, register=False, is_default=False
                )
            else:
                midm_model = load_midm_model(register=False, is_default=False)

            self.local_llm = midm_model.get_langchain_model()
            print("[OK] ë¡œì»¬ Midm LLM ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as local_error:
            error_msg = str(local_error)
            print(f"[WARNING] ë¡œì»¬ Midm ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {error_msg[:200]}...")
            import traceback

            print(f"[DEBUG] ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()[:500]}")
            self.local_llm = None

        if not self.openai_llm and not self.local_llm:
            raise RuntimeError(
                "OpenAIì™€ ë¡œì»¬ LLM ëª¨ë¸ ëª¨ë‘ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. "
                "OpenAI API í‚¤ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ Midm ëª¨ë¸ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            )

    def create_rag_chain(self, llm_model: Any, embeddings_model: Any) -> Runnable:
        """RAG ì²´ì¸ ìƒì„± - LangChain ì²´ì¸ ê¸°ëŠ¥ í™œìš©.

        Args:
            llm_model: LLM ëª¨ë¸
            embeddings_model: Embedding ëª¨ë¸

        Returns:
            RAG ì²´ì¸
        """
        try:
            # 1. Retriever ìƒì„± (í˜„ì¬ Embedding ëª¨ë¸ ì‚¬ìš©)
            current_vector_store = PGVector(
                embedding_function=embeddings_model,
                collection_name=self.collection_name,
                connection_string=self.connection_string,
            )
            retriever = current_vector_store.as_retriever(search_kwargs={"k": 3})

            # 2. ëŒ€í™” ê¸°ë¡ì„ ê³ ë ¤í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± í”„ë¡¬í”„íŠ¸
            contextualize_q_system_prompt = (
                "ëŒ€í™” ê¸°ë¡ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì´ ì£¼ì–´ì¡Œì„ ë•Œ, "
                "ëŒ€í™” ê¸°ë¡ì˜ ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ ë…ë¦½ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”. "
                "ì§ˆë¬¸ì— ë‹µí•˜ì§€ ë§ê³ , í•„ìš”ì‹œ ì¬êµ¬ì„±í•˜ê³  ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”."
            )
            contextualize_q_prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", contextualize_q_system_prompt),
                    MessagesPlaceholder("chat_history"),
                    ("human", "{input}"),
                ]
            )

            # 3. ëŒ€í™” ê¸°ë¡ì„ ê³ ë ¤í•œ Retriever ìƒì„±
            history_aware_retriever = create_history_aware_retriever(
                llm_model, retriever, contextualize_q_prompt
            )

            # 4. ì§ˆë¬¸ ë‹µë³€ í”„ë¡¬í”„íŠ¸
            qa_system_prompt = (
                "ë‹¹ì‹ ì€ LangChainê³¼ PGVectorë¥¼ ì‚¬ìš©í•˜ëŠ” ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
                "ë‹¤ìŒ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. "
                "ì»¨í…ìŠ¤íŠ¸ì— ë‹µë³€í•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì´ë©´, ì •ì¤‘í•˜ê²Œ ê·¸ë ‡ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”. "
                "ë‹µë³€ì€ ìµœëŒ€ 3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\n"
                "ì»¨í…ìŠ¤íŠ¸:\n{context}"
            )
            qa_prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", qa_system_prompt),
                    MessagesPlaceholder("chat_history"),
                    ("human", "{input}"),
                ]
            )

            # 5. ë¬¸ì„œ ê²°í•© ì²´ì¸ ìƒì„±
            question_answer_chain = create_stuff_documents_chain(llm_model, qa_prompt)

            # 6. ìµœì¢… RAG ì²´ì¸ ìƒì„±
            rag_chain = create_retrieval_chain(
                history_aware_retriever, question_answer_chain
            )

            return rag_chain
        except Exception as e:
            error_msg = str(e)
            print(f"[ERROR] RAG ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {error_msg[:200]}...")
            raise

    def initialize_rag_chain(self) -> None:
        """RAG ì²´ì¸ ì´ˆê¸°í™” - OpenAIì™€ ë¡œì»¬ ëª¨ë¸ìš© ì²´ì¸ ìƒì„±."""
        # OpenAIìš© RAG ì²´ì¸ ìƒì„±
        if self.openai_llm and self.openai_embeddings:
            try:
                self.openai_rag_chain = self.create_rag_chain(
                    self.openai_llm, self.openai_embeddings
                )
                print("[OK] OpenAI RAG ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"[WARNING] OpenAI RAG ì²´ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)[:100]}...")
                self.openai_rag_chain = None

        # ë¡œì»¬ ëª¨ë¸ìš© RAG ì²´ì¸ ìƒì„±
        if self.local_llm and self.local_embeddings:
            try:
                self.local_rag_chain = self.create_rag_chain(
                    self.local_llm, self.local_embeddings
                )
                print("[OK] ë¡œì»¬ RAG ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"[WARNING] ë¡œì»¬ RAG ì²´ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)[:100]}...")
                self.local_rag_chain = None

        if not self.openai_rag_chain and not self.local_rag_chain:
            error_msg = "OpenAIì™€ ë¡œì»¬ RAG ì²´ì¸ ëª¨ë‘ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n"
            error_msg += "ìµœì†Œ í•˜ë‚˜ì˜ LLMê³¼ Embedding ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
            print(f"[ERROR] {error_msg}")
            raise RuntimeError(error_msg)

    def chat_with_rag(
        self,
        message: str,
        history: Optional[List[Dict[str, str]]] = None,
        model_type: str = "openai",
    ) -> str:
        """RAG ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ìƒì„±.

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            history: ëŒ€í™” ê¸°ë¡
            model_type: ëª¨ë¸ íƒ€ì… ("openai" ë˜ëŠ” "local")

        Returns:
            ìƒì„±ëœ ì‘ë‹µ
        """
        # ëª¨ë¸ íƒ€ì… ì •ê·œí™”
        if model_type:
            model_type = model_type.lower()
        if model_type == "midm":
            model_type = "local"

        # ì ì ˆí•œ RAG ì²´ì¸ ì„ íƒ
        if model_type == "openai":
            if not self.openai_rag_chain:
                if self.openai_quota_exceeded:
                    raise RuntimeError("OpenAI API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    raise RuntimeError("OpenAI RAG ì²´ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            current_rag_chain = self.openai_rag_chain
        elif model_type == "local":
            if not self.local_rag_chain:
                raise RuntimeError("ë¡œì»¬ RAG ì²´ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            current_rag_chain = self.local_rag_chain
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…ì…ë‹ˆë‹¤: {model_type}")

        # ëŒ€í™” ê¸°ë¡ì„ LangChain ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        chat_history = []
        if history:
            for msg in history:
                if msg.get("role") == "user":
                    chat_history.append(HumanMessage(content=msg.get("content", "")))
                elif msg.get("role") == "assistant":
                    chat_history.append(AIMessage(content=msg.get("content", "")))

        # RAG ì²´ì¸ ì‹¤í–‰
        result = current_rag_chain.invoke(
            {
                "input": message,
                "chat_history": chat_history,
            }
        )

        # ì²´ì¸ ê²°ê³¼ì—ì„œ ë‹µë³€ ì¶”ì¶œ
        response_text = result.get("answer", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # response_textê°€ Noneì´ê±°ë‚˜ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
        if response_text is None:
            response_text = "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            response_text = str(response_text)

        # ì‘ë‹µì—ì„œ ì´ì „ ëŒ€í™” ë‚´ìš© ì œê±° (ì¤‘ë³µ ë°©ì§€)
        if response_text and (
            "Human:" in response_text or "Assistant:" in response_text
        ):
            # ë¹ ë¥¸ ì •ê·œì‹ìœ¼ë¡œ ë§ˆì§€ë§‰ Assistant: ì´í›„ë§Œ ì¶”ì¶œ
            assistant_match = re.search(
                r"Assistant:\s*(.+?)(?:\nHuman:|$)", response_text, re.DOTALL
            )
            if assistant_match:
                response_text = assistant_match.group(1).strip()

        # ë¹ˆ ì‘ë‹µ ë°©ì§€
        if not response_text or not response_text.strip():
            response_text = "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        return response_text


class ChatServiceQLoRA:
    """QLoRAë¥¼ ì‚¬ìš©í•œ ì±„íŒ… ë° í•™ìŠµ ì„œë¹„ìŠ¤."""

    def __init__(
        self,
        model_name_or_path: str,
        output_dir: str = "./qlora_output",
        lora_r: int = 16,
        lora_alpha: int = 32,
        lora_dropout: float = 0.05,
        target_modules: Optional[List[str]] = None,
        device_map: str = "auto",
    ):
        """QLoRA ì±„íŒ… ì„œë¹„ìŠ¤ ì´ˆê¸°í™”.

        Args:
            model_name_or_path: ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ
            output_dir: í•™ìŠµ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
            lora_r: LoRA rank
            lora_alpha: LoRA alpha
            lora_dropout: LoRA dropout
            target_modules: LoRAë¥¼ ì ìš©í•  ëª¨ë“ˆ ëª©ë¡ (Noneì´ë©´ ìë™ ê°ì§€)
            device_map: ë””ë°”ì´ìŠ¤ ë§¤í•‘ ("auto", "cpu", "cuda" ë“±)
        """
        self.model_name_or_path = model_name_or_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # QLoRA ì„¤ì • (4-bit quantization)
        self.bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )

        # LoRA ì„¤ì •
        if target_modules is None:
            # ì¼ë°˜ì ì¸ ëª¨ë¸ì˜ attention ëª¨ë“ˆ (Llama, Mistral ë“±)
            target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]

        self.lora_config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )

        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer: Optional[AutoTokenizer] = None
        self.model: Optional[Any] = None
        self.peft_model: Optional[PeftModel] = None
        self.device_map = device_map

        # ì„¸ì…˜ë³„ ëŒ€í™” íˆìŠ¤í† ë¦¬
        self.chat_sessions: Dict[str, List[Dict[str, str]]] = {}

    def load_model(self) -> None:
        """ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ."""
        print(f"[INFO] ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name_or_path}")

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            self.model_name_or_path,
            trust_remote_code=True,
        )

        # pad_token ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id

        self.tokenizer = tokenizer

        # ëª¨ë¸ ë¡œë“œ (4-bit quantization)
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name_or_path,
            quantization_config=self.bnb_config,
            device_map=self.device_map,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
        )

        # PEFT ëª¨ë¸ ì¤€ë¹„
        model = prepare_model_for_kbit_training(model)

        # LoRA ì ìš©
        peft_model = get_peft_model(model, self.lora_config)
        peft_model.print_trainable_parameters()

        self.model = model
        self.peft_model = peft_model

        print("[OK] ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    def load_peft_model(self, peft_model_path: str) -> None:
        """í•™ìŠµëœ PEFT ëª¨ë¸ ë¡œë“œ.

        Args:
            peft_model_path: PEFT ëª¨ë¸ ê²½ë¡œ
        """
        if self.model is None:
            raise RuntimeError("ë¨¼ì € load_model()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")

        print(f"[INFO] PEFT ëª¨ë¸ ë¡œë”© ì¤‘: {peft_model_path}")
        self.peft_model = PeftModel.from_pretrained(
            self.model, peft_model_path, device_map=self.device_map
        )
        print("[OK] PEFT ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    def chat(
        self,
        message: str,
        session_id: str = "default",
        history: Optional[List[Dict[str, str]]] = None,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
    ) -> str:
        """ëŒ€í™” ìƒì„±.

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            session_id: ì„¸ì…˜ ID
            history: ëŒ€í™” ê¸°ë¡ (Noneì´ë©´ ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì‚¬ìš©)
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒì„± ì˜¨ë„
            top_p: nucleus sampling íŒŒë¼ë¯¸í„°

        Returns:
            ìƒì„±ëœ ì‘ë‹µ
        """
        if self.peft_model is None:
            raise RuntimeError("ë¨¼ì € load_model() ë˜ëŠ” load_peft_model()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")

        # ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        if history is None:
            history = self.chat_sessions.get(session_id, [])

        # ëŒ€í™” í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._format_chat_prompt(message, history)

        # í† í¬ë‚˜ì´ì§•
        if self.tokenizer is None:
            raise RuntimeError("í† í¬ë‚˜ì´ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        inputs = self.tokenizer(
            prompt, return_tensors="pt", truncation=True, max_length=2048
        ).to(self.peft_model.device)

        # ìƒì„±
        with torch.no_grad():
            outputs = self.peft_model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id
                if self.tokenizer.pad_token_id
                else self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        # ë””ì½”ë”©
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # ì‘ë‹µë§Œ ì¶”ì¶œ (í”„ë¡¬í”„íŠ¸ ì œì™¸)
        response = generated_text[len(prompt) :].strip()

        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.chat_sessions[session_id] = history + [
            {"role": "user", "content": message},
            {"role": "assistant", "content": response},
        ]

        return response

    def _format_chat_prompt(self, message: str, history: List[Dict[str, str]]) -> str:
        """ëŒ€í™” í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±.

        Args:
            message: í˜„ì¬ ë©”ì‹œì§€
            history: ëŒ€í™” ê¸°ë¡

        Returns:
            í¬ë§·ëœ í”„ë¡¬í”„íŠ¸
        """
        prompt_parts = []

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        prompt_parts.append("ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.")

        # íˆìŠ¤í† ë¦¬ ì¶”ê°€
        for msg in history:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            if role == "user":
                prompt_parts.append(f"ì‚¬ìš©ì: {content}")
            elif role == "assistant":
                prompt_parts.append(f"ì–´ì‹œìŠ¤í„´íŠ¸: {content}")

        # í˜„ì¬ ë©”ì‹œì§€ ì¶”ê°€
        prompt_parts.append(f"ì‚¬ìš©ì: {message}")
        prompt_parts.append("ì–´ì‹œìŠ¤í„´íŠ¸:")

        return "\n".join(prompt_parts)

    def train(
        self,
        training_data: List[Dict[str, str]],
        output_dir: Optional[str] = None,
        num_epochs: int = 3,
        per_device_train_batch_size: int = 4,
        gradient_accumulation_steps: int = 4,
        learning_rate: float = 2e-4,
        warmup_steps: int = 100,
        logging_steps: int = 10,
        save_steps: int = 500,
        max_seq_length: int = 512,
    ) -> str:
        """QLoRA í•™ìŠµ ì‹¤í–‰.

        Args:
            training_data: í•™ìŠµ ë°ì´í„° ({"instruction": "...", "input": "...", "output": "..."} í˜•ì‹)
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            num_epochs: ì—í­ ìˆ˜
            per_device_train_batch_size: ë°°ì¹˜ í¬ê¸°
            gradient_accumulation_steps: ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…
            learning_rate: í•™ìŠµë¥ 
            warmup_steps: ì›Œë°ì—… ìŠ¤í…
            logging_steps: ë¡œê¹… ìŠ¤í…
            save_steps: ì €ì¥ ìŠ¤í…
            max_seq_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´

        Returns:
            í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
        """
        if self.peft_model is None:
            raise RuntimeError("ë¨¼ì € load_model()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")

        if self.tokenizer is None:
            raise RuntimeError("í† í¬ë‚˜ì´ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        output_dir = output_dir or str(
            self.output_dir / f"checkpoint-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
        )

        # ë°ì´í„°ì…‹ ì¤€ë¹„
        def format_prompt(example):
            """í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…."""
            instruction = example.get("instruction", "")
            input_text = example.get("input", "")
            output = example.get("output", "")

            if input_text:
                prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}"
            else:
                prompt = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"

            return {"text": prompt}

        dataset = Dataset.from_list(training_data)
        dataset = dataset.map(format_prompt)

        # í•™ìŠµ ì¸ì ì„¤ì •
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=per_device_train_batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            learning_rate=learning_rate,
            warmup_steps=warmup_steps,
            logging_steps=logging_steps,
            save_steps=save_steps,
            save_total_limit=3,
            fp16=False,  # QLoRAëŠ” bfloat16 ì‚¬ìš©
            bf16=True,
            optim="paged_adamw_8bit",
            lr_scheduler_type="cosine",
            report_to="none",
        )

        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer, mlm=False
        )

        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        trainer_kwargs: Dict[str, Any] = {
            "model": self.peft_model,
            "train_dataset": dataset,
            "peft_config": self.lora_config,
            "tokenizer": self.tokenizer,
            "args": training_args,
            "data_collator": data_collator,
            "max_seq_length": max_seq_length,
        }

        # packing íŒŒë¼ë¯¸í„°ëŠ” ë²„ì „ì— ë”°ë¼ ì„ íƒì 
        try:
            trainer = SFTTrainer(**trainer_kwargs, packing=False)  # type: ignore
        except TypeError:
            # packing íŒŒë¼ë¯¸í„°ê°€ ì—†ëŠ” ê²½ìš°
            trainer_kwargs.pop("packing", None)
            trainer = SFTTrainer(**trainer_kwargs)  # type: ignore

        # í•™ìŠµ ì‹¤í–‰
        print("[INFO] í•™ìŠµ ì‹œì‘...")
        trainer.train()
        print("[OK] í•™ìŠµ ì™„ë£Œ")

        # ëª¨ë¸ ì €ì¥
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)

        print(f"[OK] ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir}")
        return output_dir

    def train_from_chat_history(
        self,
        session_ids: Optional[List[str]] = None,
        output_dir: Optional[str] = None,
        **train_kwargs,
    ) -> str:
        """ì±„íŒ… íˆìŠ¤í† ë¦¬ë¡œë¶€í„° í•™ìŠµ ë°ì´í„° ìƒì„± ë° í•™ìŠµ.

        Args:
            session_ids: í•™ìŠµí•  ì„¸ì…˜ ID ëª©ë¡ (Noneì´ë©´ ëª¨ë“  ì„¸ì…˜)
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            **train_kwargs: train() ë©”ì„œë“œì— ì „ë‹¬í•  ì¶”ê°€ ì¸ì

        Returns:
            í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
        """
        # í•™ìŠµ ë°ì´í„° ìƒì„±
        training_data = []

        if session_ids is None:
            session_ids = list(self.chat_sessions.keys())

        for session_id in session_ids:
            history = self.chat_sessions.get(session_id, [])
            if len(history) < 2:
                continue

            # ëŒ€í™” ìŒìœ¼ë¡œ ë³€í™˜
            for i in range(0, len(history) - 1, 2):
                if i + 1 < len(history):
                    user_msg = history[i].get("content", "")
                    assistant_msg = history[i + 1].get("content", "")

                    training_data.append(
                        {
                            "instruction": "ë‹¤ìŒ ëŒ€í™”ì— ì‘ë‹µí•˜ì„¸ìš”.",
                            "input": user_msg,
                            "output": assistant_msg,
                        }
                    )

        if not training_data:
            raise ValueError("í•™ìŠµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        print(f"[INFO] {len(training_data)}ê°œì˜ í•™ìŠµ ìƒ˜í”Œ ìƒì„±ë¨")

        # í•™ìŠµ ì‹¤í–‰
        return self.train(training_data, output_dir=output_dir, **train_kwargs)

    def save_session(self, session_id: str, file_path: str) -> None:
        """ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì €ì¥.

        Args:
            session_id: ì„¸ì…˜ ID
            file_path: ì €ì¥ ê²½ë¡œ
        """
        history = self.chat_sessions.get(session_id, [])
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
        print(f"[OK] ì„¸ì…˜ ì €ì¥ ì™„ë£Œ: {file_path}")

    def load_session(self, session_id: str, file_path: str) -> None:
        """ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ë¡œë“œ.

        Args:
            session_id: ì„¸ì…˜ ID
            file_path: ë¡œë“œ ê²½ë¡œ
        """
        with open(file_path, "r", encoding="utf-8") as f:
            history = json.load(f)
        self.chat_sessions[session_id] = history
        print(f"[OK] ì„¸ì…˜ ë¡œë“œ ì™„ë£Œ: {file_path}")

    def clear_session(self, session_id: str) -> None:
        """ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì‚­ì œ.

        Args:
            session_id: ì„¸ì…˜ ID
        """
        if session_id in self.chat_sessions:
            del self.chat_sessions[session_id]
            print(f"[OK] ì„¸ì…˜ ì‚­ì œ ì™„ë£Œ: {session_id}")

    def get_session_history(self, session_id: str) -> List[Dict[str, str]]:
        """ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°.

        Args:
            session_id: ì„¸ì…˜ ID

        Returns:
            ëŒ€í™” ê¸°ë¡ ë¦¬ìŠ¤íŠ¸
        """
        return self.chat_sessions.get(session_id, [])
